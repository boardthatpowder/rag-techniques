{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval: Re-ranking\n",
    " \n",
    "![re-ranking](../images/images-re-ranking.png)\n",
    "\n",
    "**Re-ranking** is a process used in information retrieval systems to improve the relevance of search results. After an initial set of documents or items is retrieved based on a user's query, re-ranking involves reordering these results to better match the user's intent.\n",
    "\n",
    "**How Re-ranking Works:**\n",
    "\n",
    "1. **Initial Retrieval:**\n",
    "   - The system retrieves a broad set of documents or items that potentially match the user's query.\n",
    "\n",
    "2. **Scoring:**\n",
    "   - Each retrieved item is evaluated using advanced models or criteria to assess its relevance to the query.\n",
    "\n",
    "3. **Re-ordering:**\n",
    "   - Based on the new relevance scores, the items are reordered so that the most pertinent results appear at the top.\n",
    "\n",
    "**Benefits of Re-ranking:**\n",
    "\n",
    "- **Enhanced Relevance:** By refining the order of search results, re-ranking ensures that users are presented with the most relevant information first.\n",
    "- **Improved User Satisfaction:** Delivering more accurate results increases user satisfaction and the overall effectiveness of the retrieval system.\n",
    "\n",
    "**Example in Practice:**\n",
    "\n",
    "Imagine you're searching for \"best Italian restaurants in New York.\" The initial search might retrieve a wide range of Italian restaurants. Re-ranking would then reorder these results to prioritize those with the highest ratings, best reviews, or closest proximity, ensuring that the top suggestions align closely with your intent.\n",
    "\n",
    "Incorporating re-ranking into retrieval systems enhances their ability to deliver precise and user-centric results, making information access more efficient and intuitive. \n",
    "\n",
    "![re-ranking](../images/re-ranking.png)\n",
    "\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'enable_langsmith' (bool)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "%run \"../Z - Common/setup.ipynb\"\n",
    "\n",
    "!pip install -qU cohere dill==0.3.8 multiprocess==0.70.16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = load_sample_data()\n",
    "split_docs = split_sample_data(docs)\n",
    "retriever = seed_sample_data(split_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We touched on this subject earlier when we looked at RAG-Fusion where we implemented a _reciprocal rank fusion_ function to re-rank results. But lets look at how to achieve the same using [Cohere Re-Rank](https://python.langchain.com/docs/integrations/retrievers/cohere-reranker#doing-reranking-with-coherererank). \n",
    "\n",
    "Links:\n",
    "\n",
    "- Cohere [blog](https://cohere.com/blog/rerank)\n",
    "\n",
    "Let's start by defining the prompt and chain to retrieve related documents (in the same way as for the RAG-fusion example):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "template = \"\"\"You are a helpful assistant that generates multiple search queries based on a single input query. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output a total of 4 queries. Just the queries as text, nothing else, separated via new line character:\"\"\"\n",
    "\n",
    "prompt_generate_queries = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "chain_generate_queries = (\n",
    "    prompt_generate_queries \n",
    "    | llm\n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")\n",
    "\n",
    "chain_retrieval = (\n",
    "    chain_generate_queries \n",
    "    | retriever.map()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass the results, as text, to the Cohere reranker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence:  1\n",
      "Relevance score:  0.8312667\n",
      "Fig. 1. Overview of a LLM-powered autonomous agent system.\n",
      "Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
      "Task Decomposition#\n",
      "Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\n",
      "Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
      "Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n",
      "\n",
      "\n",
      "Sequence:  2\n",
      "Relevance score:  0.035002798\n",
      "The AI assistant can parse user input to several tasks: [{\"task\": task, \"id\", task_id, \"dep\": dependency_task_ids, \"args\": {\"text\": text, \"image\": URL, \"audio\": URL, \"video\": URL}}]. The \"dep\" field denotes the id of the previous task which generates a new resource that the current task relies on. A special tag \"-task_id\" refers to the generated text image, audio and video in the dependency task with id as task_id. The task MUST be selected from the following options: {{ Available Task List }}. There is a logical relationship between tasks, please note their order. If the user input can't be parsed, you need to reply empty JSON. Here are several cases for your reference: {{ Demonstrations }}. The chat history is recorded as {{ Chat History }}. From this chat history, you can find the path of the user-mentioned resources for your task planning.\n",
      "\n",
      "(2) Model selection: LLM distributes the tasks to expert models, where the request is framed as a multiple-choice question. LLM is presented with a list of models to choose from. Due to the limited context length, task type based filtration is needed.\n",
      "Instruction:\n",
      "\n",
      "\n",
      "Sequence:  3\n",
      "Relevance score:  0.035002798\n",
      "The AI assistant can parse user input to several tasks: [{\"task\": task, \"id\", task_id, \"dep\": dependency_task_ids, \"args\": {\"text\": text, \"image\": URL, \"audio\": URL, \"video\": URL}}]. The \"dep\" field denotes the id of the previous task which generates a new resource that the current task relies on. A special tag \"-task_id\" refers to the generated text image, audio and video in the dependency task with id as task_id. The task MUST be selected from the following options: {{ Available Task List }}. There is a logical relationship between tasks, please note their order. If the user input can't be parsed, you need to reply empty JSON. Here are several cases for your reference: {{ Demonstrations }}. The chat history is recorded as {{ Chat History }}. From this chat history, you can find the path of the user-mentioned resources for your task planning.\n",
      "\n",
      "(2) Model selection: LLM distributes the tasks to expert models, where the request is framed as a multiple-choice question. LLM is presented with a list of models to choose from. Due to the limited context length, task type based filtration is needed.\n",
      "Instruction:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.load import dumps\n",
    "import cohere\n",
    "\n",
    "llm_reranker = cohere.BedrockClientV2(\n",
    "    aws_access_key=os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "    aws_secret_key=os.environ[\"AWS_SECRET_ACCESS_KEY\"],\n",
    "    aws_session_token=os.environ[\"AWS_SESSION_TOKEN\"], \n",
    "    aws_region=os.environ[\"AWS_REGION\"]\n",
    ")\n",
    "\n",
    "question = \"What is task decomposition?\"\n",
    "\n",
    "docs = chain_retrieval.invoke({\"question\": question})\n",
    "# print(\"docs: \", docs)\n",
    "\n",
    "docs_str = [dumps(doc) for doc in docs]\n",
    "\n",
    "reranked_docs = llm_reranker.rerank(\n",
    "    model=\"cohere.rerank-v3-5:0\",\n",
    "    query=question,\n",
    "    documents=docs_str,\n",
    "    top_n=3,\n",
    ")\n",
    "\n",
    "# print(reranked_docs)\n",
    "\n",
    "for idx, doc in enumerate(reranked_docs.results):\n",
    "    # print(doc)\n",
    "    print(\"Sequence: \",  idx + 1)\n",
    "    print(\"Relevance score: \", doc.relevance_score)\n",
    "    print(docs[doc.index][0].page_content)\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-techniques",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
