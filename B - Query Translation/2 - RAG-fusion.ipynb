{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Translation: RAG-Fusion\n",
    "\n",
    "![rag-fusion](../images/images-rag-fusion.png)\n",
    "\n",
    "**RAG-Fusion** is a technique that focuses on improving the quality of retrieved information by combining the results of multiple retrieval attempts into a single, optimized list. Let’s break it down step by step:\n",
    "\n",
    "### The Problem RAG-Fusion Solves\n",
    "\n",
    "When we ask a RAG system a question, it retrieves documents based on the query to help generate a response. Sometimes, one retrieval attempt isn’t enough because:\n",
    "\n",
    "- Different ways of phrasing the query might retrieve different relevant documents.\n",
    "- A single query might miss important information.\n",
    "\n",
    "RAG-Fusion solves this by using **multiple queries** and then combining their results intelligently.\n",
    "\n",
    "### How RAG-Fusion Works\n",
    "\n",
    "1. **Generate Multiple Queries**:\n",
    "\n",
    "   - The system creates several versions of the original query (e.g., rephrased, summarized, or expanded).\n",
    "   - Example:\n",
    "     - Original Query: \"How to secure APIs?\"\n",
    "     - Alternate Queries:\n",
    "       - \"API security guidelines\"\n",
    "       - \"Best practices for API authentication\"\n",
    "       - \"Protecting APIs from attacks\"\n",
    "\n",
    "2. **Retrieve Documents for Each Query**:\n",
    "\n",
    "   - For each query version, the system retrieves relevant documents from the knowledge base.\n",
    "\n",
    "3. **Fuse the Results**:\n",
    "\n",
    "   - The retrieved documents are combined into a single list.\n",
    "   - Fusion gives priority to documents that appear in multiple queries or rank highly in individual retrievals.\n",
    "   - This step eliminates duplicates and ensures the final list is more relevant and diverse.\n",
    "\n",
    "4. **Feed into the LLM**:\n",
    "\n",
    "   - The fused list of documents is passed to the LLM, along with the user’s original query, for generating a high-quality response.\n",
    "\n",
    "\n",
    "### Why RAG-Fusion Works\n",
    "\n",
    "- **Increases Coverage**: By exploring different ways to ask the question, it captures more relevant information.\n",
    "- **Improves Quality**: Fusion ensures that the best and most relevant documents are included.\n",
    "- **Reduces Bias**: It avoids over-reliance on a single phrasing of the query.\n",
    "\n",
    "### Simple Example\n",
    "\n",
    "#### Query:\n",
    "\n",
    "*\"What are the best ways to secure an API?\"*\n",
    "\n",
    "#### Without RAG-Fusion:\n",
    "\n",
    "- The system retrieves only the top documents based on the exact query. It might miss some important resources.\n",
    "\n",
    "#### With RAG-Fusion:\n",
    "\n",
    "1. Multiple Queries:\n",
    "   - \"Best API security practices\"\n",
    "   - \"API authentication methods\"\n",
    "   - \"Securing REST APIs\"\n",
    "\n",
    "2. Retrieved Documents:\n",
    "   - From Query 1: Document A, Document B.\n",
    "   - From Query 2: Document B, Document C.\n",
    "   - From Query 3: Document D, Document A.\n",
    "\n",
    "3. Fused List:\n",
    "   - Prioritize Document A and Document B (they appear multiple times).\n",
    "   - Include Document C and Document D for broader coverage.\n",
    "\n",
    "4. Final Output:\n",
    "   - A more accurate and comprehensive response because the system considered diverse perspectives.\n",
    "\n",
    "### Key Benefits of RAG-Fusion\n",
    "\n",
    "- **More Reliable Answers**: By merging results, it ensures no crucial information is overlooked.\n",
    "- **Flexible Querying**: Works well even if the user’s original query is vague or incomplete.\n",
    "- **Enhanced Relevance**: The fused list prioritizes the best sources for generating a response.\n",
    "\n",
    "In short, RAG-Fusion is like asking the same question in different ways to gather more complete and accurate information, then using the best of those results to provide a great answer!\n",
    "\n",
    "![rag fusion](../images/rag_fusion.png)\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"../Z - Common/setup.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = load_sample_data()\n",
    "split_docs = split_sample_data(docs)\n",
    "retriever = seed_sample_data(split_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by defining the prompt and chain to retrieve related documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "template = \"\"\"You are a helpful assistant that generates multiple search queries based on a single input query. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (4 queries):\"\"\"\n",
    "\n",
    "prompt_generate_queries = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "chain_generate_queries = (\n",
    "    prompt_generate_queries \n",
    "    | llm\n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAG-Fusion applies a reciprocal rank fusion function to rerank the results. Let's define the function, and build a chain that calls the function after the different versions of the query have been processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.load import dumps, loads\n",
    "\n",
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    \"\"\" Reciprocal_rank_fusion that takes multiple lists of ranked documents \n",
    "        and an optional parameter k used in the RRF formula \"\"\"\n",
    "    \n",
    "    # Initialize a dictionary to hold fused scores for each unique document\n",
    "    fused_scores = {}\n",
    "\n",
    "    # Iterate through each list of ranked documents\n",
    "    for docs in results:\n",
    "        # Iterate through each document in the list, with its rank (position in the list)\n",
    "        for rank, doc in enumerate(docs):\n",
    "            # Convert the document to a string format to use as a key (assumes documents can be serialized to JSON)\n",
    "            doc_str = dumps(doc)\n",
    "            # If the document is not yet in the fused_scores dictionary, add it with an initial score of 0\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            # Update the score of the document using the RRF formula: 1 / (rank + k)\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "\n",
    "    # Sort the documents based on their fused scores in descending order to get the final reranked results\n",
    "    reranked_results = [\n",
    "        (loads(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "\n",
    "    # Return the reranked results as a list of tuples, each containing the document and its fused score\n",
    "    return reranked_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_retrieval = (\n",
    "    chain_generate_queries \n",
    "    | retriever.map() \n",
    "    | reciprocal_rank_fusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='}\\n]\\nChallenges#\\nAfter going through key ideas and demos of building LLM-centered agents, I start to see a couple common limitations:'),\n",
       "  0.05),\n",
       " (Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory'),\n",
       "  0.016666666666666666),\n",
       " (Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Fig. 13. The generative agent architecture. (Image source: Park et al. 2023)\\nThis fun simulation results in emergent social behavior, such as information diffusion, relationship memory (e.g. two agents continuing the conversation topic) and coordination of social events (e.g. host a party and invite many others).\\nProof-of-Concept Examples#\\nAutoGPT has drawn a lot of attention into the possibility of setting up autonomous agents with LLM as the main controller. It has quite a lot of reliability issues given the natural language interface, but nevertheless a cool proof-of-concept demo. A lot of code in AutoGPT is about format parsing.\\nHere is the system message used by AutoGPT, where {{...}} are user inputs:\\nYou are {{ai-name}}, {{user-provided AI bot description}}.\\nYour decisions must always be made independently without seeking user assistance. Play to your strengths as an LLM and pursue simple strategies with no legal complications.\\n\\nGOALS:\\n\\n1. {{user-provided goal 1}}\\n2. {{user-provided goal 2}}\\n3. ...\\n4. ...\\n5. ...'),\n",
       "  0.016666666666666666),\n",
       " (Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='HNSW (Hierarchical Navigable Small World): It is inspired by the idea of small world networks where most nodes can be reached by any other nodes within a small number of steps; e.g. “six degrees of separation” feature of social networks. HNSW builds hierarchical layers of these small-world graphs, where the bottom layers contain the actual data points. The layers in the middle create shortcuts to speed up search. When performing a search, HNSW starts from a random node in the top layer and navigates towards the target. When it can’t get any closer, it moves down to the next layer, until it reaches the bottom layer. Each move in the upper layers can potentially cover a large distance in the data space, and each move in the lower layers refines the search quality.\\nFAISS (Facebook AI Similarity Search): It operates on the assumption that in high dimensional space, distances between nodes follow a Gaussian distribution and thus there should exist clustering of data points. FAISS applies vector quantization by partitioning the vector space into clusters and then refining the quantization within clusters. Search first looks for cluster candidates with coarse quantization and then further looks into each cluster with finer quantization.'),\n",
       "  0.016666666666666666)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What are the main components of an LLM-powered autonomous agent system?\"\n",
    "\n",
    "docs = chain_retrieval.invoke({\"question\": question})\n",
    "print(len(docs))\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now build the final chain that after retrieving the results for the different variations of the query and re-ranking, it then uses those documents as context to answer the original question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/deanhart/miniconda3/envs/rag-techniques/lib/python3.11/site-packages/langsmith/client.py:256: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "chain_rag = (\n",
    "    {\"context\": chain_retrieval, \n",
    "     \"question\": itemgetter(\"question\")} \n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the context, a LLM-powered autonomous agent system consists of three main components: (1) LLM as the core controller or \"brain,\" (2) Planning capabilities, which include subgoal decomposition and reflection/refinement mechanisms, and (3) Memory systems. The LLM functions as the primary decision-maker while planning helps break down complex tasks and memory enables the system to retain and learn from past experiences.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = chain_rag.invoke({\"question\":question})\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the [Langchain tracing](https://smith.langchain.com/) to understand the flow better.\n",
    "\n",
    "Save the results so we can compare later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_results(\"rag-fusion.txt\", result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-techniques",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
