{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Translation: HyDE (Hypothetical Document Embeddings)\n",
    "\n",
    "![hyDE](../images/images-hyde.png)\n",
    "\n",
    "**HyDE** (Hypothetical Document Embeddings) is a technique that enhances retrieval by first generating a hypothetical document based on the user query, and then using it to find real documents in the knowledge base. It bridges the gap when direct retrieval from the query might fail or return irrelevant results.\n",
    "\n",
    "### The Problem HyDE Solves\n",
    "Sometimes, user queries:\n",
    "- Are vague or lack enough information for effective retrieval.\n",
    "- Don’t have direct matches in the knowledge base, leading to poor or no results.\n",
    "\n",
    "**HyDE** solves this by creating a hypothetical document—a kind of imagined answer or context—that guides the retrieval system toward the most relevant documents.\n",
    "\n",
    "### How HyDE Works\n",
    "1. **Generate a Hypothetical Document**:\n",
    "   - Use an LLM to generate a plausible response or context based on the user query.\n",
    "   - This document doesn’t need to be accurate—it just needs to be similar to what the user might be looking for.\n",
    "   - Example:\n",
    "     - Query: \"How does token expiry work in OAuth 2.0?\"\n",
    "     - Hypothetical Document:\n",
    "       - \"Token expiry in OAuth 2.0 ensures that access tokens are valid only for a short duration, reducing the risk of unauthorized access.\"\n",
    "\n",
    "2. **Create Embeddings**:\n",
    "   - Convert the hypothetical document into a vector representation (embedding) using a dense embedding model.\n",
    "   - This vector captures the semantic meaning of the hypothetical document.\n",
    "\n",
    "3. **Retrieve Real Documents**:\n",
    "   - Use the embedding of the hypothetical document to search the knowledge base for semantically similar real documents.\n",
    "   - Example:\n",
    "     - Retrieved documents might include OAuth 2.0 specifications, API security guidelines, or examples of token expiry implementation.\n",
    "\n",
    "4. **Generate the Final Response**:\n",
    "   - Combine the real documents retrieved with the user query and pass them to the LLM for generating a final, accurate response.\n",
    "\n",
    "### Why HyDE Works\n",
    "- **Fills Knowledge Gaps**: By hypothesizing an answer, HyDE helps retrieve relevant information even when the query itself lacks enough detail.\n",
    "- **Improves Retrieval Quality**: The hypothetical document aligns better with the information in the knowledge base, guiding the retriever effectively.\n",
    "- **Handles Vague Queries**: Works well for open-ended or poorly defined questions.\n",
    "\n",
    "### Simple Example\n",
    "#### Query:\n",
    "*\"What is the role of token expiry in OAuth 2.0 security?\"*\n",
    "\n",
    "#### Without HyDE:\n",
    "- The system might struggle to find documents if the exact query terms (e.g., \"token expiry\") aren’t present in the indexed documents.\n",
    "\n",
    "#### With HyDE:\n",
    "1. **Generate Hypothetical Document**:\n",
    "   - \"Token expiry in OAuth 2.0 ensures short-lived access, preventing misuse and enabling safer API interactions.\"\n",
    "2. **Create Embedding**:\n",
    "   - Generate a semantic embedding for the hypothetical document.\n",
    "3. **Retrieve Real Documents**:\n",
    "   - Use the embedding to find documents about OAuth 2.0 and token-based security.\n",
    "   - Retrieved documents might include detailed OAuth workflows or articles on API security.\n",
    "4. **Generate the Final Response**:\n",
    "   - Combine the retrieved documents and user query to produce a response:\n",
    "     - \"Token expiry in OAuth 2.0 limits the duration an access token is valid, reducing the risk of token misuse. Refresh tokens are used to renew access without compromising security.\"\n",
    "\n",
    "### Key Benefits of HyDE\n",
    "- **Resilient to Query Limitations**: Generates relevant results even if the query lacks specificity or direct matches.\n",
    "- **Semantic Accuracy**: Embedding the hypothetical document ensures retrieval is based on meaning, not just keywords.\n",
    "- **Flexible Application**: Can handle complex, open-ended, or poorly phrased queries.\n",
    "\n",
    "In short, **HyDE** is like imagining a possible answer to your question first, and then using that imagined answer to find the best real-world information to back it up!\n",
    "\n",
    "![hyde](../images/hyde.png)\n",
    "\n",
    "Arxiv paper:\n",
    "\n",
    "- [Precise Zero-Shot Dense Retrieval without Relevance Labels](https://arxiv.org/pdf/2212.10496)\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'enable_langsmith' (bool)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "%run \"../Z - Common/setup.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = load_sample_data()\n",
    "split_docs = split_sample_data(docs)\n",
    "retriever = seed_sample_data(split_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by building the prompts and chain to retrieve the documents:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "template = \"\"\"Please write a scientific paper passage to answer the question\n",
    "Question: {question}\n",
    "Passage:\"\"\"\n",
    "\n",
    "prompt_hyde = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "chain_generate_docs_for_retrieval = (\n",
    "    prompt_hyde | llm | StrOutputParser() \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Here's a scientific paper passage addressing the main components of an LLM-powered autonomous agent system:\\n\\nThe Architecture of LLM-Powered Autonomous Agent Systems: A Component Analysis\\n\\nThe fundamental architecture of Large Language Model (LLM)-powered autonomous agent systems typically comprises several essential components that work in concert to enable intelligent, goal-directed behavior. These core components can be categorized into five primary subsystems:\\n\\n1. Language Model Core\\nThe foundation of the system is the Large Language Model itself, which serves as the cognitive engine. This component handles natural language understanding, reasoning, and generation capabilities. Modern implementations typically utilize transformer-based architectures with parameters ranging from billions to trillions, enabling complex reasoning and decision-making processes.\\n\\n2. Memory Management System\\nThe memory architecture consists of multiple layers:\\n- Short-term working memory for immediate task processing\\n- Episodic memory for storing past experiences and interactions\\n- Semantic memory for maintaining factual knowledge\\n- Procedural memory for action patterns and learned behaviors\\nThese memory systems enable contextual awareness and learning from past experiences.\\n\\n3. Planning and Executive Control\\nThis component manages:\\n- Goal decomposition and task planning\\n- Action selection and prioritization\\n- Resource allocation and optimization\\n- Error detection and recovery mechanisms\\nThe planning system translates high-level objectives into actionable steps while maintaining strategic alignment with overall goals.\\n\\n4. Environment Interface Layer\\nThis crucial component facilitates:\\n- Input processing from various sources (text, API calls, sensors)\\n- Output generation and action execution\\n- Environment state monitoring\\n- Tool usage and integration capabilities\\nThe interface layer enables the agent to interact with its operational environment and external systems.\\n\\n5. Self-Monitoring and Regulation System\\nThis meta-cognitive component includes:\\n- Performance monitoring and evaluation\\n- Error detection and correction mechanisms\\n- Resource usage optimization\\n- Safety constraints and ethical guidelines enforcement\\n\\nThese components are interconnected through a complex network of information flows and control mechanisms, allowing for dynamic adaptation and learning. The effectiveness of an autonomous agent system largely depends on the seamless integration and coordination of these core components.\\n\\nRecent research has demonstrated that the robustness and reliability of autonomous agent systems are significantly enhanced when these components are properly implemented and balanced, with particular attention to the interactions between the planning system and the memory management architecture (Johnson et al., 2023).\\n\\nThe success of these systems relies heavily on the careful orchestration of these components, with each playing a vital role in the agent's ability to understand, reason, plan, and act in complex environments while maintaining goal-directed behavior and adapting to changing circumstances.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What are the main components of an LLM-powered autonomous agent system?\"\n",
    "chain_generate_docs_for_retrieval.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next build the retriever chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_retriever = chain_generate_docs_for_retrieval | retriever \n",
    "\n",
    "retrieved_docs = chain_retriever.invoke({\"question\":question})\n",
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally build the RAG chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/deanhart/miniconda3/envs/rag-techniques/lib/python3.11/site-packages/langsmith/client.py:256: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "chain_rag = (\n",
    "    prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the context, the main components of an LLM-powered autonomous agent system include the LLM itself functioning as the brain, a planning component (which handles subgoal decomposition and task breakdown), and a memory system. The planning component also includes reflection and refinement capabilities, allowing the agent to learn from past actions and improve future performance.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result=chain_rag.invoke({\"context\":retrieved_docs,\"question\":question})\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the results so we can compare later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_results(\"hyde.txt\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-techniques",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
