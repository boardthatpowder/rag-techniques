{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Common Setup\n",
    "\n",
    "Config common to all RAG examples is configured here.\n",
    "\n",
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -qU jupyter-contrib-nbextensions pickleshare\n",
    "\n",
    "! pip install -qU langchain_aws langchain_community tiktoken langchain chromadb langchain-chroma\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangSmith\n",
    "\n",
    "Setting the following LangSmith environment variables allows the use of [LangSmith tracing](https://smith.langchain.com/). To use this you need a LangSmith API key (requires a free account creating). This is optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "def _set_env(key: str):\n",
    "    if key not in os.environ:\n",
    "        os.environ[key] = getpass.getpass(f\"{key}:\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "try:\n",
    "    enable_langsmith\n",
    "except NameError:\n",
    "    enable_langsmith = input('Enabled Langsmith tracing? (y/n): ').lower().strip() == 'y'\n",
    "    %store enable_langsmith\n",
    "\n",
    "if (enable_langsmith=='y'):\n",
    "    os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "    os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "    _set_env(\"LANGCHAIN_API_KEY\")\n",
    "else:\n",
    "    os.environ['LANGCHAIN_TRACING_V2'] = 'false'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "_set_env(\"AWS_ACCESS_KEY_ID\")\n",
    "_set_env(\"AWS_SECRET_ACCESS_KEY\")\n",
    "_set_env(\"AWS_SESSION_TOKEN\")\n",
    "os.environ[\"AWS_REGION\"] = 'us-west-2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws import BedrockEmbeddings, ChatBedrock\n",
    "import os\n",
    "\n",
    "embeddings = BedrockEmbeddings()\n",
    "\n",
    "llm = ChatBedrock(\n",
    "    aws_access_key_id=os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "    aws_secret_access_key=os.environ[\"AWS_SECRET_ACCESS_KEY\"],\n",
    "    aws_session_token=os.environ[\"AWS_SESSION_TOKEN\"], \n",
    "    region_name=os.environ[\"AWS_REGION\"],\n",
    "    model_id=\"anthropic.claude-3-5-sonnet-20241022-v2:0\",\n",
    "    model_kwargs={\"temperature\": 0}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare functions to load data from a blog, split it, then load the split data into a vectorstore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.documents import Document\n",
    "from typing import Iterator\n",
    "import bs4\n",
    "\n",
    "def load_sample_data() -> Iterator[Document]:\n",
    "    \"\"\"Loads data from a blog, intended to be later stored in a vectorstore.\"\"\"\n",
    "\n",
    "    loader = WebBaseLoader(\n",
    "        web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "        bs_kwargs=dict(\n",
    "            parse_only=bs4.SoupStrainer(\n",
    "                class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "            )\n",
    "        ),\n",
    "    )\n",
    "    docs = loader.load()\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from typing import List\n",
    "\n",
    "def split_sample_data(docs:Iterator[Document], chunk_size=300, chunk_overlap=50) -> List[Document]:\n",
    "    \"\"\"Splits the text\"\"\"\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=chunk_size, \n",
    "        chunk_overlap=chunk_overlap)\n",
    "\n",
    "    # Make splits\n",
    "    splits = text_splitter.split_documents(docs)\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_core.vectorstores import VectorStoreRetriever\n",
    "from uuid import uuid4\n",
    "\n",
    "def seed_sample_data(documents:List[Document], k=1) -> VectorStoreRetriever: \n",
    "    \"\"\"Creates and seeds a vectorstore\"\"\"\n",
    "    \n",
    "    vector_store = Chroma(\n",
    "        collection_name=\"rag_techniques\",\n",
    "        embedding_function=embeddings,\n",
    "        persist_directory=\"./chroma_db\",\n",
    "    )\n",
    "\n",
    "    uuids = [str(uuid4()) for _ in range(len(documents))]\n",
    "    vector_store.add_documents(documents=documents, ids=uuids)\n",
    "    \n",
    "    retriever = vector_store.as_retriever(search_kwargs={\"k\": k})\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Tiktoken](https://github.com/openai/tiktoken/blob/main/README.md) is a fast [BPE](https://en.wikipedia.org/wiki/Byte_pair_encoding) open-source tokenizer by OpenAI. Given a text string (e.g., `tiktoken is great!`) and an encoding (e.g., `cl100k_base`), a tokenizer can split the text string into a list of tokens (e.g., [`t`, `ik`, `token`, `is`, `great`, `!`]). Splitting text strings into tokens is useful because GPT models see text in the form of tokens. Knowing how many tokens are in a text string can tell you (a) whether the string is too long for a text model to process and (b) how much an OpenAI API call costs (as usage is priced by token)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_results(filename, result):\n",
    "    \"\"\"Write results to disk for later analysis\"\"\"\n",
    "    \n",
    "    os.makedirs(\"../Z - results\", exist_ok=True)\n",
    "\n",
    "    with open(\"../Z - results/\" + filename, \"w\") as f:\n",
    "        f.write(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-techniques",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
