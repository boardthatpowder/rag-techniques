{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rag Techniques\n",
    "\n",
    "This notebook walks through the process of building RAG app(s) from scratch. They will build towards a broader understanding of the RAG landscape, as shown below. Note this notebook is a refactor of the [OpenAI Rag From Scratch repo](https://github.com/langchain-ai/rag-from-scratch) but simplified, and modified to support Bedrock.\n",
    "\n",
    "![RAG Landscape - full](images/rag_landscape_full.png)\n",
    "\n",
    "## Environment Setup\n",
    "\n",
    "`(1) Packages`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "transformers 4.47.1 requires tokenizers<0.22,>=0.21, but you have tokenizers 0.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "chromadb 0.5.23 requires tokenizers<=0.20.3,>=0.13.2, but you have tokenizers 0.21.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "Usage:   \n",
      "  pip uninstall [options] <package> ...\n",
      "  pip uninstall [options] -r <requirements file> ...\n",
      "\n",
      "no such option: -U\n"
     ]
    }
   ],
   "source": [
    "! pip install -qU langchain_community tiktoken langchain_aws langchainhub chromadb langchain arxiv pymupdf langgraph \n",
    "! pip install -qU matplotlib scikit-learn pandas umap\n",
    "! pip install -qU ragatouille\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(2) LangSmith`\n",
    "\n",
    "Setting the following LangSmith environment variables allows the use of [LangSmith tracing](https://smith.langchain.com/). To use this you need a LangSmith API key (requires a free account creating). This is optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = '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'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(3) AWS Credentials`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = '<UPDATE_THIS>'\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = '<UPDATE_THIS>'\n",
    "os.environ[\"AWS_SESSION_TOKEN\"] = '<UPDATE_THIS>'\n",
    "os.environ[\"AWS_REGION\"] = 'us-west-2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(4) Imports`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as Soup\n",
    "from IPython.display import Image, display\n",
    "from langchain import hub\n",
    "from langchain_aws import BedrockEmbeddings, ChatBedrock\n",
    "from langchain_community.document_loaders import WebBaseLoader, ArxivLoader, RecursiveUrlLoader\n",
    "from langchain_community.tools.sql_database.tool import QuerySQLDatabaseTool\n",
    "from langchain_community.utilities import SQLDatabase\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain.globals import set_debug\n",
    "from langchain.load import dumps, loads\n",
    "from langchain.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate, PromptTemplate\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.storage import InMemoryByteStore\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.utils.math import cosine_similarity\n",
    "from langgraph.graph import START, StateGraph\n",
    "from operator import itemgetter\n",
    "from pprint import pprint\n",
    "from pydantic import BaseModel, Field\n",
    "from ragatouille import RAGPretrainedModel\n",
    "from ragatouille.utils import get_wikipedia_page\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from typing import Literal, Optional, Dict, List, Optional, Tuple\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "import bs4\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import umap\n",
    "import uuid\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic RAG\n",
    "### Basic Indexing\n",
    "\n",
    "![indexing](images/indexing.png)\n",
    "\n",
    "Let's define a simple question and document to seed the database with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### INDEXING - documents ####\n",
    "\n",
    "question = \"What kinds of pets do I like?\"\n",
    "document = \"My favorite pet is a cat.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Tiktoken](https://github.com/openai/tiktoken/blob/main/README.md) is a fast [BPE](https://en.wikipedia.org/wiki/Byte_pair_encoding) open-source tokenizer by OpenAI. Given a text string (e.g., `tiktoken is great!`) and an encoding (e.g., `cl100k_base`), a tokenizer can split the text string into a list of tokens (e.g., [`t`, `ik`, `token`, `is`, `great`, `!`]). Splitting text strings into tokens is useful because GPT models see text in the form of tokens. Knowing how many tokens are in a text string can tell you (a) whether the string is too long for a text model to process and (b) how much an OpenAI API call costs (as usage is priced by token)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### INDEXING - utility function ####\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "num_tokens_from_string(question, \"cl100k_base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data needs embedding first before it can be used by an LLM and/or vector data store. The [BedrockEmbeddings](https://python.langchain.com/api_reference/aws/embeddings/langchain_aws.embeddings.bedrock.BedrockEmbeddings.html#langchain_aws.embeddings.bedrock.BedrockEmbeddings) uses `amazon.titan-embed-text-v1` by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### INDEXING - embeddings ####\n",
    "\n",
    "embeddings = BedrockEmbeddings()\n",
    "query_result = embeddings.embed_query(question)\n",
    "document_result = embeddings.embed_query(document)\n",
    "len(query_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[DocumentLoaders](https://python.langchain.com/docs/integrations/document_loaders/) load data into the standard LangChain Document format. Each DocumentLoader has its own specific parameters, but they can all be invoked in the same way with the .load method.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### INDEXING - loading documents ####\n",
    "\n",
    "# Load blog\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "blog_docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before loading the documents into vector store they need to be [split](https://python.langchain.com/docs/how_to/recursive_text_splitter/). Here we are starting off with basic splitting by length (`300` characters with `50` overlap), but later will explore other splitting techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### INDEXING - splitting ####\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=300, \n",
    "    chunk_overlap=50)\n",
    "\n",
    "# Make splits\n",
    "splits = text_splitter.split_documents(blog_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then finally we can load the split embeddings into the [vector store](https://python.langchain.com/docs/integrations/vectorstores/). For simplicity we are using [Chroma](https://python.langchain.com/docs/integrations/vectorstores/chroma/) as the vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### INDEXING - splitting ####\n",
    "\n",
    "vector_store = Chroma.from_documents(documents=splits, \n",
    "                                    embedding=BedrockEmbeddings())\n",
    "\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Retrieval\n",
    "\n",
    "Using a `retriever` we can query the vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### RETRIEVAL ####\n",
    "\n",
    "docs = retriever.invoke(\"What is Task Decomposition?\")\n",
    "\n",
    "print(\"No. of results: \", len(docs))\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Generation\n",
    "\n",
    "![generation](images/generation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the prompt we will pass to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GENERATION - prompt ####\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt_basic = ChatPromptTemplate.from_template(template)\n",
    "prompt_basic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the LLM.\n",
    "\n",
    "> Note that langchain does not yet seem to support the new Amazon `Nova` models (see [JIRA](https://github.com/langchain-ai/langchain-aws/issues/308) ticket). Once support has been added we can try Nova models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GENERATION - llm ####\n",
    "\n",
    "llm_claude_3_5_sonnet_v2 = ChatBedrock(\n",
    "    aws_access_key_id=os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "    aws_secret_access_key=os.environ[\"AWS_SECRET_ACCESS_KEY\"],\n",
    "    aws_session_token=os.environ[\"AWS_SESSION_TOKEN\"], \n",
    "    region_name=os.environ[\"AWS_REGION\"],\n",
    "    model_id=\"anthropic.claude-3-5-sonnet-20241022-v2:0\",\n",
    "    model_kwargs={\"temperature\": 0}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build  a basic chain. The `|` operator [chains runnable objects](https://python.langchain.com/docs/how_to/sequence/) (objects that have an `invoke()` function) together so as one object is streaming output, the next object in the chain can receive the stream as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GENERATION - basic chain ####\n",
    "\n",
    "chain_basic = prompt_basic | llm_claude_3_5_sonnet_v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the chain by calling its invoke method. The `dict` passed to `invoke()` is used to tokenize varibles declared at any of the steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GENERATION - basic chain ####\n",
    "\n",
    "chain_basic.invoke({\"context\":docs,\"question\":\"What is Task Decomposition?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of defining our own prompts, we can make use of prompt templates published in the [Langchain Hub](https://smith.langchain.com/hub). Lets replace our previous prompt with one from the hub and rebuild the chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GENERATION - basic chain using langchain hub prompt ####\n",
    "\n",
    "prompt_basic = hub.pull(\"rlm/rag-prompt\")\n",
    "pprint(prompt_basic)\n",
    "\n",
    "chain_basic = prompt_basic | llm_claude_3_5_sonnet_v2\n",
    "chain_basic.invoke({\"context\":docs, \"question\":\"What is Task Decomposition?\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now build a basic RAG chain, where instead of explicitly passing `docs` as the context we instead provide the `retriever` to query the vector store directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GENERATION - basic rag chain ####\n",
    "\n",
    "chain_basic_rag = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt_basic\n",
    "    | llm_claude_3_5_sonnet_v2\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain_basic_rag.invoke(\"What is Task Decomposition?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Translation\n",
    "### Multi Query\n",
    "\n",
    "![multi-query](images/multi-query.png)\n",
    "\n",
    "Basic RAG chains find similar embedded documents based on a distance metric. But, retrieval may produce different results with subtle changes in query wording, or if the embeddings do not capture the semantics of the data well. Prompt engineering / tuning is sometimes done to manually address these problems, but can be tedious.\n",
    "\n",
    "The [MultiQueryRetriever](https://python.langchain.com/docs/how_to/MultiQueryRetriever/) automates the process of prompt tuning by using an LLM to generate multiple queries from different perspectives for a given user input query. For each query, it retrieves a set of relevant documents and takes the unique union across all queries to get a larger set of potentially relevant documents. By generating multiple perspectives on the same question, the MultiQueryRetriever can mitigate some of the limitations of the distance-based retrieval and get a richer set of results.\n",
    "\n",
    "First let's build a prompt that enables multi-query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### QUERY TRANSLATION - multi-query prompt ####\n",
    "\n",
    "template = \"\"\"You are an AI language model assistant. Your task is to generate five \n",
    "different versions of the given user question to retrieve relevant documents from a vector \n",
    "database. By generating multiple perspectives on the user question, your goal is to help\n",
    "the user overcome some of the limitations of the distance-based similarity search. \n",
    "Provide these alternative questions separated by newlines. Original question: {question}\"\"\"\n",
    "\n",
    "prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "chain_multi_query_generate_queries = (\n",
    "    prompt_perspectives \n",
    "    | llm_claude_3_5_sonnet_v2   \n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the results from the 5 different variations of the original query, we need to union the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### QUERY TRANSLATION - multi-query utility function ####\n",
    "\n",
    "def get_unique_union(documents: list[list]):\n",
    "    \"\"\" Unique union of retrieved docs \"\"\"\n",
    "    # Flatten list of lists, and convert each Document to string\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    # Get unique documents\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    # Return\n",
    "    return [loads(doc) for doc in unique_docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now build a second chain that joins the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### QUERY TRANSLATION - multi-query retrieve ####\n",
    "\n",
    "chain_multi_query_retrieval = chain_multi_query_generate_queries | retriever.map() | get_unique_union\n",
    "\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "docs = chain_multi_query_retrieval.invoke({\"question\":question})\n",
    "\n",
    "print(\"No. of results: \", len(docs))\n",
    "docs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the [Langchain tracing](https://smith.langchain.com/) for the request we just executed. You will see the call to the LLM was provided the following input:\n",
    "\n",
    "```\n",
    "You are an AI language model assistant. Your task is to generate five \n",
    "different versions of the given user question to retrieve relevant documents from a vector \n",
    "database. By generating multiple perspectives on the user question, your goal is to help\n",
    "the user overcome some of the limitations of the distance-based similarity search. \n",
    "Provide these alternative questions separated by newlines. Original question: What is task decomposition for LLM agents?\n",
    "```\n",
    "\n",
    "And rendered the following output:\n",
    "\n",
    "```\n",
    "Here are 5 alternative versions of the question to help retrieve relevant documents:\n",
    "\n",
    "How do LLM agents break down complex tasks into smaller subtasks?\n",
    "\n",
    "What are the methods and techniques used for decomposing tasks when working with language model agents?\n",
    "\n",
    "Can you explain the process of splitting larger problems into manageable steps for AI agents?\n",
    "\n",
    "What is the role of task planning and decomposition in LLM-based autonomous systems?\n",
    "\n",
    "How do large language model agents analyze and structure tasks into hierarchical components?\n",
    "```\n",
    "\n",
    "It then made 5 queries to the vector store to answer those questions, and joined (union) the results.\n",
    "\n",
    "Finally, we build a third chain once has retrieved the results from the previous chain, uses those documents as context to answer the original question:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### QUERY TRANSLATION - multi-query RAG ####\n",
    "\n",
    "chain_multi_query_rag = (\n",
    "    {\"context\": chain_multi_query_retrieval, \n",
    "     \"question\": itemgetter(\"question\")} \n",
    "    | prompt_basic\n",
    "    | llm_claude_3_5_sonnet_v2\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain_multi_query_rag.invoke({\"question\":question})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a look at the [Langchain tracing](https://smith.langchain.com/) again for the request we just executed, you will see two calls. The first call is the same as the one we just previously described. The second call provides the results from the vector store as context to the LLM to answer the original question.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG-Fusion\n",
    "\n",
    "![rag fusion](images/rag_fusion.png)\n",
    "\n",
    "RAG-Fusion takes multi-query one step further. It employs multiple query generation just like multi-query, but then uses Reciprocal Rank Fusion to re-ran the search results.\n",
    "\n",
    "Let's start by defining the prompt and chain to retrieve related documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### QUERY TRANSLATION - RAG-Fusion: Related ####\n",
    "\n",
    "template = \"\"\"You are a helpful assistant that generates multiple search queries based on a single input query. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (4 queries):\"\"\"\n",
    "\n",
    "prompt_rag_fusion = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "chain_rag_fusion_generate_queries = (\n",
    "    prompt_rag_fusion \n",
    "    | llm_claude_3_5_sonnet_v2\n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\\n\"))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned previously, RAG-Fusion applies a reciprocal rank fusion function to rerank the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### QUERY TRANSLATION - utility function ####\n",
    "\n",
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    \"\"\" Reciprocal_rank_fusion that takes multiple lists of ranked documents \n",
    "        and an optional parameter k used in the RRF formula \"\"\"\n",
    "    \n",
    "    # Initialize a dictionary to hold fused scores for each unique document\n",
    "    fused_scores = {}\n",
    "\n",
    "    # Iterate through each list of ranked documents\n",
    "    for docs in results:\n",
    "        # Iterate through each document in the list, with its rank (position in the list)\n",
    "        for rank, doc in enumerate(docs):\n",
    "            # Convert the document to a string format to use as a key (assumes documents can be serialized to JSON)\n",
    "            doc_str = dumps(doc)\n",
    "            # If the document is not yet in the fused_scores dictionary, add it with an initial score of 0\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            # Retrieve the current score of the document, if any\n",
    "            previous_score = fused_scores[doc_str]\n",
    "            # Update the score of the document using the RRF formula: 1 / (rank + k)\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "\n",
    "    # Sort the documents based on their fused scores in descending order to get the final reranked results\n",
    "    reranked_results = [\n",
    "        (loads(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "\n",
    "    # Return the reranked results as a list of tuples, each containing the document and its fused score\n",
    "    return reranked_results\n",
    "\n",
    "chain_rag_fusion_retrieval = chain_rag_fusion_generate_queries | retriever.map() | reciprocal_rank_fusion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = chain_rag_fusion_retrieval.invoke({\"question\": question})\n",
    "print(len(docs))\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now build the final chain that once has retrieved the results for the different variations of the query, then applies the re-ranking function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### QUERY TRANSLATION - RAG-Fusion: RAG chain ####\n",
    "\n",
    "chain_rag_fusion_retrieval_rag = (\n",
    "    {\"context\": chain_rag_fusion_retrieval, \n",
    "     \"question\": itemgetter(\"question\")} \n",
    "    | prompt_basic\n",
    "    | llm_claude_3_5_sonnet_v2\n",
    "    | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_rag_fusion_retrieval_rag.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the [Langchain tracing](https://smith.langchain.com/) to understand the flow better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decomposition\n",
    "\n",
    "_Decomposition_ breaks down a question into multiple sub-questions, then proceeds to answer each question before having the context to answer the original question.\n",
    "\n",
    "There are 2 variations - answering the decomposed questions recursively, or answering individually. Let's start by looking at the recursive option first.\n",
    "\n",
    "![decomposition_recursive](images/decomposition_recursive.png)\n",
    "\n",
    "\n",
    "\n",
    "Arxiv papers:\n",
    "\n",
    "- [Least-to-Most Prompting Enables Complex Reasoning in Large Language Models](https://arxiv.org/pdf/2205.10625)\n",
    "- [Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions](https://arxiv.org/pdf/2212.10509)\n",
    "\n",
    "Let's start by creating the prompt and chain to generate the initial decomposed sub-questions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### QUERY TRANSLATION - Decomposition - decompose question ####\n",
    "\n",
    "template = \"\"\"You are a helpful assistant that generates multiple sub-questions related to an input question. \\n\n",
    "The goal is to break down the input into a set of sub-problems / sub-questions that can be answers in isolation. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (3 queries):\"\"\"\n",
    "\n",
    "prompt_decomposition_decompose = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "chain_decomposition_generate_queries = ( \n",
    "                                        prompt_decomposition_decompose \n",
    "                                        | llm_claude_3_5_sonnet_v2\n",
    "                                        | StrOutputParser() \n",
    "                                        | (lambda x: x.split(\"\\n\\n\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### QUERY TRANSLATION - Decomposition - decompose question ####\n",
    "\n",
    "question = \"What are the main components of an LLM-powered autonomous agent system?\"\n",
    "decomposed_questions = chain_decomposition_generate_queries.invoke({\"question\":question})\n",
    "\n",
    "decomposed_questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the prompt to answer the original question based on the additional context gathered via the decomposition chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### QUERY TRANSLATION - Decomposition - recursive prompt ####\n",
    "\n",
    "template = \"\"\"Here is the question you need to answer:\n",
    "\n",
    "\\n --- \\n {question} \\n --- \\n\n",
    "\n",
    "Here is any available background question + answer pairs:\n",
    "\n",
    "\\n --- \\n {q_a_pairs} \\n --- \\n\n",
    "\n",
    "Here is additional context relevant to the question: \n",
    "\n",
    "\\n --- \\n {context} \\n --- \\n\n",
    "\n",
    "Use the above context and any background question + answer pairs to answer the question: \\n {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt_decomposition_solve_recursively = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define an additional chain that takes the `decomposed_questions` and solves using the above prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### QUERY TRANSLATION - Decomposition - utility function ####\n",
    "\n",
    "def format_qa_pair_recursive(question, answer):\n",
    "    \"\"\"Format Q and A pair\"\"\"\n",
    "    \n",
    "    formatted_string = \"\"\n",
    "    formatted_string += f\"Question: {question}\\nAnswer: {answer}\\n\\n\"\n",
    "    return formatted_string.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### QUERY TRANSLATION - Decomposition - RAG recursive ####\n",
    "\n",
    "def decomposition_retrieve_and_rag_recursive(decomposed_questions):\n",
    "    q_a_pairs = \"\"\n",
    "    for q in decomposed_questions:\n",
    "        \n",
    "        rag_chain = (\n",
    "            {\"context\": itemgetter(\"question\") | retriever, \n",
    "            \"question\": itemgetter(\"question\"),\n",
    "            \"q_a_pairs\": itemgetter(\"q_a_pairs\")} \n",
    "            | prompt_decomposition_solve_recursively\n",
    "            | llm_claude_3_5_sonnet_v2\n",
    "            | StrOutputParser())\n",
    "\n",
    "        answer = rag_chain.invoke({\"question\":q,\"q_a_pairs\":q_a_pairs})\n",
    "        q_a_pair = format_qa_pair_recursive(q,answer)\n",
    "        q_a_pairs = q_a_pairs + \"\\n---\\n\"+  q_a_pair\n",
    "        \n",
    "    return (answer, q_a_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer,q_a_pairs = decomposition_retrieve_and_rag_recursive(decomposed_questions)\n",
    "\n",
    "print(answer)\n",
    "print(q_a_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now take a look at how to perform decomposition individually.\n",
    "\n",
    "![decomposition_individually](images/decomposition_individually.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### QUERY TRANSLATION - Decomposition - RAG individually ####\n",
    "\n",
    "def decomposition_retrieve_and_rag_individually(question,prompt_rag,sub_question_generator_chain):\n",
    "    \"\"\"RAG on each sub-question\"\"\"\n",
    "    \n",
    "    # Use our decomposition / \n",
    "    sub_questions = sub_question_generator_chain.invoke({\"question\":question})\n",
    "    \n",
    "    # Initialize a list to hold RAG chain results\n",
    "    rag_results = []\n",
    "    \n",
    "    for sub_question in sub_questions:\n",
    "        \n",
    "        # Retrieve documents for each sub-question\n",
    "        retrieved_docs = retriever.invoke(sub_question)\n",
    "        \n",
    "        # Use retrieved documents and sub-question in RAG chain\n",
    "        answer = (\n",
    "            prompt_rag \n",
    "            | llm_claude_3_5_sonnet_v2 \n",
    "            | StrOutputParser()).invoke({\"context\": retrieved_docs, \"question\": sub_question})\n",
    "        rag_results.append(answer)\n",
    "    \n",
    "    return rag_results,sub_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### QUERY TRANSLATION - Decomposition - RAG individually ####\n",
    "\n",
    "# Wrap the retrieval and RAG process in a RunnableLambda for integration into a chain\n",
    "answers, questions = decomposition_retrieve_and_rag_individually(question, prompt_basic, chain_decomposition_generate_queries)\n",
    "\n",
    "print(\"Answers:\")\n",
    "print(answers)\n",
    "\n",
    "print(\"Questions:\")\n",
    "print(questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the final chain to perform decompostion individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### QUERY TRANSLATION - Decomposition - utility function ####\n",
    "\n",
    "def format_qa_pairs_individually(questions, answers):\n",
    "    \"\"\"Format Q and A pairs\"\"\"\n",
    "    \n",
    "    formatted_string = \"\"\n",
    "    for i, (question, answer) in enumerate(zip(questions, answers), start=1):\n",
    "        formatted_string += f\"Question {i}: {question}\\nAnswer {i}: {answer}\\n\\n\"\n",
    "    return formatted_string.strip()\n",
    "\n",
    "context = format_qa_pairs_individually(questions, answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### QUERY TRANSLATION - Decomposition - prompt an chain individually ####\n",
    "\n",
    "# Prompt\n",
    "template = \"\"\"Here is a set of Q+A pairs:\n",
    "\n",
    "{context}\n",
    "\n",
    "Use these to synthesize an answer to the question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt_decomposition_solve_individually = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "chain_decomposition_solve_individually = (\n",
    "    prompt_decomposition_solve_individually\n",
    "    | llm_claude_3_5_sonnet_v2\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=chain_decomposition_solve_individually.invoke({\"context\":context,\"question\":question})\n",
    "\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step Back\n",
    "\n",
    "Step Back prompting enables LLMs to step back and look at the bigger picture, finding general ideas and riles from specific examples. Using these ideas, the LLM can solve problems more logically and make fewer mistakes.\n",
    "\n",
    "![step back](images/step%20back.png)\n",
    "\n",
    "Arxiv papers:\n",
    "\n",
    "- [Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models](https://arxiv.org/pdf/2310.06117)\n",
    "\n",
    "Let's start by defining the prompt which includes few shot examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### QUERY TRANSLATION - Step back - prompts ####\n",
    "\n",
    "# Few Shot Examples\n",
    "step_back_examples = [\n",
    "    {\n",
    "        \"input\": \"Could the members of The Police perform lawful arrests?\",\n",
    "        \"output\": \"what can the members of The Police do?\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Jan Sindel’s was born in what country?\",\n",
    "        \"output\": \"what is Jan Sindel’s personal history?\",\n",
    "    },\n",
    "]\n",
    "# We now transform these to example messages\n",
    "prompt_step_back_example = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "prompt_step_back_few_shot = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=prompt_step_back_example,\n",
    "    examples=step_back_examples,\n",
    ")\n",
    "prompt_step_back_abstract = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are an expert at world knowledge. Your task is to step back and paraphrase a question to a more generic step-back question, which is easier to answer. Here are a few examples:\"\"\",\n",
    "        ),\n",
    "        # Few shot examples\n",
    "        prompt_step_back_few_shot,\n",
    "        # New question\n",
    "        (\"user\", \"{question}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_step_back_generate_queries = prompt_step_back_abstract | llm_claude_3_5_sonnet_v2 | StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is task decomposition for LLM agents?\"\n",
    "chain_step_back_generate_queries.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### QUERY TRANSLATION - Step back - RAG ####\n",
    "\n",
    "# Response prompt \n",
    "template = \"\"\"You are an expert of world knowledge. I am going to ask you a question. Your response should be comprehensive and not contradicted with the following context if they are relevant. Otherwise, ignore them if they are not relevant.\n",
    "\n",
    "# {normal_context}\n",
    "# {step_back_context}\n",
    "\n",
    "# Original Question: {question}\n",
    "# Answer:\"\"\"\n",
    "prompt_step_back_solve = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "chain_step_back_rag = (\n",
    "    {\n",
    "        # Retrieve context using the normal question\n",
    "        \"normal_context\": RunnableLambda(lambda x: x[\"question\"]) | retriever,\n",
    "        # Retrieve context using the step-back question\n",
    "        \"step_back_context\": chain_step_back_generate_queries | retriever,\n",
    "        # Pass on the question\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "    }\n",
    "    | prompt_step_back_solve\n",
    "    | llm_claude_3_5_sonnet_v2\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_step_back_rag.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HyDE\n",
    "\n",
    "HyDE (Hypothetical Document Embeddings) is an embedding technique that takes queries, generates a hypothetical answer, and then embeds that generated document and uses that as the final example.\n",
    "\n",
    "![hyde](images/hyde.png)\n",
    "\n",
    "Arxiv paper:\n",
    "\n",
    "- [Precise Zero-Shot Dense Retrieval without Relevance Labels](https://arxiv.org/pdf/2212.10496)\n",
    "\n",
    "Let's start by building the prompts and chain to retrieve the documents:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### QUERY TRANSLATION - HyDE - retrieval prompt and chain ####\n",
    "\n",
    "template = \"\"\"Please write a scientific paper passage to answer the question\n",
    "Question: {question}\n",
    "Passage:\"\"\"\n",
    "\n",
    "prompt_hyde = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "chain_hyde_generate_docs_for_retrieval = (\n",
    "    prompt_hyde | llm_claude_3_5_sonnet_v2 | StrOutputParser() \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is task decomposition for LLM agents?\"\n",
    "chain_hyde_generate_docs_for_retrieval.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next build the retriever chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### QUERY TRANSLATION - HyDE - retriever chain ####\n",
    "\n",
    "chain_hyde_retriever = chain_hyde_generate_docs_for_retrieval | retriever \n",
    "\n",
    "hyde_retrieved_docs = chain_hyde_retriever.invoke({\"question\":question})\n",
    "hyde_retrieved_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally build the RAG chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### QUERY TRANSLATION - HyDE - RAG ####\n",
    "\n",
    "chain_hyde_rag = (\n",
    "    prompt_basic\n",
    "    | llm_claude_3_5_sonnet_v2\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain_hyde_rag.invoke({\"context\":hyde_retrieved_docs,\"question\":question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Routing\n",
    "### Logical routing\n",
    "\n",
    "Logical routing uses function-calling for classification.\n",
    "\n",
    "![logical routing](images/logical_routing.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data model\n",
    "class RouteQuery(BaseModel):\n",
    "    \"\"\"Route a user query to the most relevant datasource.\"\"\"\n",
    "\n",
    "    datasource: Literal[\"python_docs\", \"js_docs\", \"golang_docs\"] = Field(\n",
    "        ...,\n",
    "        description=\"Given a user question choose which datasource would be most relevant for answering their question\",\n",
    "    )\n",
    "\n",
    "# LLM with function call \n",
    "# Structured Outputs is a feature that ensures the model will always generate responses that adhere to your supplied JSON Schema, \n",
    "# so you don't need to worry about the model omitting a required key, or hallucinating an invalid enum value.\n",
    "structured_llm_route_query = llm_claude_3_5_sonnet_v2.with_structured_output(RouteQuery)\n",
    "\n",
    "# Prompt \n",
    "template = \"\"\"You are an expert at routing a user question to the appropriate data source.\n",
    "\n",
    "Based on the programming language the question is referring to, route it to the relevant data source.\"\"\"\n",
    "\n",
    "prompt_routing_logical = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", template),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define router \n",
    "chain_routing_logical = prompt_routing_logical | structured_llm_route_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\"Why doesn't the following code work:\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\"human\", \"speak in {language}\"])\n",
    "prompt.invoke(\"french\")\n",
    "\"\"\"\n",
    "\n",
    "result_routing_logical = chain_routing_logical.invoke({\"question\": question})\n",
    "\n",
    "result_routing_logical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use a [custom function to route between different sub chains](https://python.langchain.com/docs/how_to/routing/#using-a-custom-function-recommended) based on the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_route(result):\n",
    "    if \"python_docs\" in result.datasource.lower():\n",
    "        ### Logic here \n",
    "        return \"chain for python_docs\"\n",
    "    elif \"js_docs\" in result.datasource.lower():\n",
    "        ### Logic here \n",
    "        return \"chain for js_docs\"\n",
    "    else:\n",
    "        ### Logic here \n",
    "        return \"golang_docs\"\n",
    "\n",
    "\n",
    "chain_routing_logical_full = chain_routing_logical | RunnableLambda(choose_route)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_routing_logical_full.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic routing\n",
    "\n",
    "Semantic routing involves using embeddings to route a query to the most relevant prompt based on semantic similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two prompts\n",
    "template_routing_semantic_physics = \"\"\"You are a very smart physics professor. \\\n",
    "You are great at answering questions about physics in a concise and easy to understand manner. \\\n",
    "When you don't know the answer to a question you admit that you don't know.\n",
    "\n",
    "Here is a question:\n",
    "{query}\"\"\"\n",
    "\n",
    "template_routing_semantic_math = \"\"\"You are a very good mathematician. You are great at answering math questions. \\\n",
    "You are so good because you are able to break down hard problems into their component parts, \\\n",
    "answer the component parts, and then put them together to answer the broader question.\n",
    "\n",
    "Here is a question:\n",
    "{query}\"\"\"\n",
    "\n",
    "# Embed prompts\n",
    "templates_routing_semantic = [template_routing_semantic_physics, template_routing_semantic_math]\n",
    "prompt_embeddings = embeddings.embed_documents(templates_routing_semantic)\n",
    "\n",
    "# Route question to prompt \n",
    "def prompt_router(input):\n",
    "    # Embed question\n",
    "    query_embedding = embeddings.embed_query(input[\"query\"])\n",
    "    # Compute similarity\n",
    "    similarity = cosine_similarity([query_embedding], prompt_embeddings)[0]\n",
    "    most_similar = templates_routing_semantic[similarity.argmax()]\n",
    "    # Chosen prompt \n",
    "    print(\"Using MATH\" if most_similar == template_routing_semantic_math else \"Using PHYSICS\")\n",
    "    return PromptTemplate.from_template(most_similar)\n",
    "\n",
    "\n",
    "chain_routing_semantic = (\n",
    "    {\"query\": RunnablePassthrough()}\n",
    "    | RunnableLambda(prompt_router)\n",
    "    | llm_claude_3_5_sonnet_v2\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_routing_semantic.invoke(\"What's a black hole\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Construction\n",
    "\n",
    "With typical RAG, a user query is converted into a vector representation. This vector is then compared to vector representations of the source documents to find the most similar ones. This works fairly well for unstructured data. Query construction is the process of converting natural language into a specific query syntax for each structured data type.\n",
    "\n",
    "Examples include [Text-to-metadata-filter](https://python.langchain.com/docs/how_to/self_query/) for Vectorstores, [Text-to-SQL](https://python.langchain.com/docs/tutorials/sql_qa/) for SQL DB, [Text-to-SQL+ Semantic](https://github.com/langchain-ai/langchain/blob/master/cookbook/retrieval_in_sql.ipynb?ref=blog.langchain.dev) for PGVector supported SQL DB, and [Text-to-Cypher](https://neo4j.com/labs/neodash/2.4/user-guide/extensions/natural-language-queries/) for graph databases.\n",
    "\n",
    "### Text-to-metadata-filter\n",
    "\n",
    "Vectorstores equipped with metadata filtering enable structured queries to filter embedded unstructured documents. The self-query retriever can translate natural language queries into these structured queries using a few steps:\n",
    "\n",
    "![text-to-metadata-filter](images/text-to-metadata-filter.png)\n",
    "\n",
    "Let's use Arxiv papers as an example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_text_to_metadata_filter = ArxivLoader(\n",
    "    query=\"reasoning\",\n",
    "    load_max_docs=5,\n",
    "    load_all_available_meta=True\n",
    ").load()\n",
    "\n",
    "print(docs_text_to_metadata_filter[0].page_content[:1000])\n",
    "print(docs_text_to_metadata_filter[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume we want to build an index that enables us to:\n",
    "\n",
    "- perform unstructured search over the `Title` and `Summary` attributes of each document\n",
    "- use range filtering on `Published`\n",
    "\n",
    "To convert a natural langauge query into a structured query we need to define a schema for the structured search queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArxivSearch(BaseModel):\n",
    "    \"\"\"Search over Arxiv documents.\"\"\"\n",
    "\n",
    "    title_search: str = Field(\n",
    "        ...,\n",
    "        description=\"Similarity search query applied to the title.\",\n",
    "    )\n",
    "    summary_search: str = Field(\n",
    "        ...,\n",
    "        description=(\n",
    "            \"Alternate version of the content search query to apply to summaries. \"\n",
    "            \"Should be succinct and only include key words that could be in a summary.\"\n",
    "        ),\n",
    "    )\n",
    "    earliest_published_date: Optional[datetime.date] = Field(\n",
    "        None,\n",
    "        description=\"Earliest published date filter, inclusive. Only use if explicitly specified.\",\n",
    "    )\n",
    "    latest_published_date: Optional[datetime.date] = Field(\n",
    "        None,\n",
    "        description=\"Latest published date filter, exclusive. Only use if explicitly specified.\",\n",
    "    )\n",
    "\n",
    "\n",
    "    def pretty_print(self) -> None:\n",
    "        for field in self.model_fields:\n",
    "            if getattr(self, field) is not None and getattr(self, field) != getattr(\n",
    "                self.model_fields[field], \"default\", None\n",
    "            ):\n",
    "                print(f\"{field}: {getattr(self, field)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we prompt the LLM to produce queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_metadata_filter_system_template = \"\"\"You are an expert at converting user questions into database queries. \\\n",
    "You have access to a database of scholarly articles. \\\n",
    "Given a question, return a database query optimized to retrieve the most relevant results.\n",
    "\n",
    "If there are acronyms or words you are not familiar with, do not try to rephrase them.\"\"\"\n",
    "text_to_metadata_filter_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", text_to_metadata_filter_system_template),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "text_to_metadata_filter_structured_llm = llm_claude_3_5_sonnet_v2.with_structured_output(ArxivSearch)\n",
    "\n",
    "chain_text_to_metadata_filter_query_analyzer = text_to_metadata_filter_prompt | text_to_metadata_filter_structured_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_text_to_metadata_filter_query_analyzer.invoke({\"question\": \"rag from scratch\"}).pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_text_to_metadata_filter_query_analyzer.invoke({\"question\": \"what papers on RAG were published in 2024?\"}).pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text-to-sql \n",
    "\n",
    "Text-to-SQL is a task in natural language processing (NLP) where the goal is to automatically generate SQL queries from natural language text. The task involves converting the text input into a structured representation and then using this representation to generate a semantically correct SQL query that can be executed on a database.\n",
    "\n",
    "Let's start by seeding a local SQLLite database with sample data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -s https://raw.githubusercontent.com/lerocha/chinook-database/master/ChinookDatabase/DataSources/Chinook_Sqlite.sql | sqlite3 Chinook.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_sql_db = SQLDatabase.from_uri(\"sqlite:///Chinook.db\")\n",
    "print(text_to_sql_db.dialect)\n",
    "print(text_to_sql_db.get_usable_table_names())\n",
    "text_to_sql_db.run(\"SELECT * FROM Artist LIMIT 10;\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a class to map state (keep track of input question, generated query, query result, and generated answer):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextToSqlState(TypedDict):\n",
    "    question: str\n",
    "    query: str\n",
    "    result: str\n",
    "    answer: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to take the user input and convert it to a SQL query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Given an input question, create a syntactically correct {dialect} query to run to help find the answer. Unless the user specifies in his question a specific \n",
    "number of examples they wish to obtain, always limit your query to at most {top_k} results. You can order the results by a relevant column to return the most interesting examples in the database.\n",
    "\n",
    "Never query for all the columns from a specific table, only ask for a the few relevant columns given the question.\n",
    "\n",
    "Pay attention to use only the column names that you can see in the schema description. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.\n",
    "\n",
    "Only use the following tables:\n",
    "{table_info}\n",
    "\n",
    "Question: {input}\n",
    "\"\"\"\n",
    "\n",
    "prompt_text_to_sql = ChatPromptTemplate.from_template(template)\n",
    "prompt_text_to_sql\n",
    "\n",
    "# Note: the langchain-ai/sql-query-system-prompt\" prompt is not compatible with Bedrock as it passes the prompt as `system` instead of `prompt`\n",
    "# prompt_text_to_sql = hub.pull(\"langchain-ai/sql-query-system-prompt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prompt includes several parameters we will need to populate, such as the SQL dialect and table schemas. LangChain's [SQLDatabase](https://python.langchain.com/api_reference/community/utilities/langchain_community.utilities.sql_database.SQLDatabase.html) object includes methods to help with this. Our `text_to_sql_write_query` step will just populate these parameters and prompt a model to generate the SQL query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_debug(False)\n",
    "\n",
    "class TextToSqlQueryOutput(TypedDict):\n",
    "    \"\"\"Generated SQL query.\"\"\"\n",
    "    query: Annotated[str, ..., \"Syntactically valid SQL query.\"]\n",
    "    \n",
    "def text_to_sql_write_query(state: TextToSqlState):\n",
    "    \"\"\"Generate SQL query to fetch information.\"\"\"\n",
    "    prompt = prompt_text_to_sql.invoke(\n",
    "        {\n",
    "            \"dialect\": text_to_sql_db.dialect,\n",
    "            \"top_k\": 10,\n",
    "            \"table_info\": text_to_sql_db.get_table_info(),\n",
    "            \"input\": state[\"question\"],\n",
    "        }\n",
    "    )\n",
    "    structured_llm = llm_claude_3_5_sonnet_v2.with_structured_output(TextToSqlQueryOutput)\n",
    "    result = structured_llm.invoke(prompt)\n",
    "    return {\"query\": result[\"query\"]}\n",
    "\n",
    "text_to_sql_write_query({\"question\": \"How many Employees are there?\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the most dangerous part of creating a SQL chain. Consider carefully if it is OK to run automated queries over your data. Minimize the database connection permissions as much as possible. Consider adding a human approval step to you chains before query execution.\n",
    "\n",
    "To execute the query, we will load a tool from [langchain-community](https://python.langchain.com/docs/concepts/architecture/#langchain-community). Our `text_to_sql_execute_query` node will just wrap this tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_sql_execute_query(state: TextToSqlState):\n",
    "    \"\"\"Execute SQL query.\"\"\"\n",
    "    execute_query_tool = QuerySQLDatabaseTool(db=text_to_sql_db)\n",
    "    return {\"result\": execute_query_tool.invoke(state[\"query\"])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our last step generates an answer to the question given the information pulled from the database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_sql_generate_answer(state: TextToSqlState):\n",
    "    \"\"\"Answer question using retrieved information as context.\"\"\"\n",
    "    prompt = (\n",
    "        \"Given the following user question, corresponding SQL query, and SQL result, answer the user question.\\n\\n\"\n",
    "        f'Question: {state[\"question\"]}\\n'\n",
    "        f'SQL Query: {state[\"query\"]}\\n'\n",
    "        f'SQL Result: {state[\"result\"]}'\n",
    "    )\n",
    "    response = llm_claude_3_5_sonnet_v2.invoke(prompt)\n",
    "    return {\"answer\": response.content}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now build the final chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_sql_graph_builder = StateGraph(TextToSqlState).add_sequence(\n",
    "    [text_to_sql_write_query, text_to_sql_execute_query, text_to_sql_generate_answer]\n",
    ")\n",
    "text_to_sql_graph_builder.add_edge(START, \"text_to_sql_write_query\")\n",
    "text_to_sql_graph = text_to_sql_graph_builder.compile()\n",
    "\n",
    "display(Image(text_to_sql_graph.get_graph().draw_mermaid_png()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our application! Note that we can stream the results of individual steps:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step in text_to_sql_graph.stream(\n",
    "    {\"question\": \"How many employees are there?\"}, stream_mode=\"updates\"\n",
    "):\n",
    "    print(step)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text-to-cypher \n",
    "\n",
    "Text-to-cypher is a task in natural language processing (NLP) where the goal is to automatically generate graph queries from natural language text. Similar to text-to-sql the task involves converting the text input into a structured representation and then using this representation to generate a semantically correct SQL query that can be executed on a database.\n",
    "\n",
    "> **TODO!**: A text-to-cypher example will required Neo4j installing. Figure out simplest way to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing\n",
    "\n",
    "### Multi-representation Indexing\n",
    "\n",
    "![Multi-representation Indexing](images/Multi-representation%20Indexing.png)\n",
    "\n",
    "Multi-representation Indexing in LLMs organizes data using multiple types of representations, such as text, images, or structured formats, to improve how information is retrieved and understood. By encoding data into different embedding spaces optimized for specific tasks or modalities, this approach enables powerful capabilities like semantic search, cross-modal retrieval (e.g., finding images using text queries), and task-specific optimizations. For instance, a technician could describe a problem in text and instantly retrieve related guides, images, or videos. This technique leverages the strengths of LLMs and other models to handle diverse data types, making retrieval more accurate, flexible, and scalable for real-world applications.\n",
    "\n",
    "Arxiv paper:\n",
    "\n",
    "- [Dense X Retrieval: What Retrieval Granularity Should We Use?](https://arxiv.org/pdf/2312.06648)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example we will use a `MultiVectorRetriever` that retrieves raw documents from an `InMemoryByteStore`, and their summaries from a vector store (Chroma).\n",
    "\n",
    "Let's start by downloading and summarizing docs to seed the database with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "docs_multi_representation_indexing = loader.load()\n",
    "\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2024-02-05-human-data-quality/\")\n",
    "docs_multi_representation_indexing.extend(loader.load())\n",
    "\n",
    "chain_multi_representation_indexing = (\n",
    "    {\"doc\": lambda x: x.page_content}\n",
    "    | ChatPromptTemplate.from_template(\"Summarize the following document:\\n\\n{doc}\")\n",
    "    | llm_claude_3_5_sonnet_v2\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "summaries_multi_representation_indexing = chain_multi_representation_indexing.batch(docs_multi_representation_indexing, {\"max_concurrency\": 5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, store the summaries in the vector store and the raw documents in the byte store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The vectorstore to use to index the child chunks\n",
    "vectorstore_multi_representation_indexing = Chroma(collection_name=\"summaries\",\n",
    "                     embedding_function=embeddings)\n",
    "\n",
    "# The storage layer for the parent documents\n",
    "byte_store_multi_representation_indexing = InMemoryByteStore()\n",
    "id_key = \"doc_id\"\n",
    "\n",
    "# The retriever\n",
    "retriever_multi_representation_indexing = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore_multi_representation_indexing,\n",
    "    byte_store=byte_store_multi_representation_indexing,\n",
    "    id_key=id_key,\n",
    ")\n",
    "doc_ids = [str(uuid.uuid4()) for _ in docs]\n",
    "\n",
    "# Docs linked to summaries\n",
    "summary_docs_multi_representation_indexing = [\n",
    "    Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "    for i, s in enumerate(summaries_multi_representation_indexing)\n",
    "]\n",
    "\n",
    "# Add\n",
    "retriever_multi_representation_indexing.vectorstore.add_documents(summary_docs_multi_representation_indexing)\n",
    "retriever_multi_representation_indexing.docstore.mset(list(zip(doc_ids, docs_multi_representation_indexing)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try querying both the summary and raw docs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Memory in agents\"\n",
    "sub_docs = vectorstore_multi_representation_indexing.similarity_search(query,k=1)\n",
    "sub_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs = retriever_multi_representation_indexing.invoke(query,n_results=1)\n",
    "retrieved_docs[0].page_content[0:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When splitting documents for retrieval, there are often conflicting desires:\n",
    "\n",
    "- You may want to have small documents, so that their embeddings can most accurately reflect their meaning. If too long, then the embeddings can lose meaning.\n",
    "- You want to have long enough documents that the context of each chunk is retained.\n",
    "\n",
    "An alternative to the approach we just implemented for multi-representation indexing is the [ParentDocumentRetriever](https://python.langchain.com/docs/how_to/parent_document_retriever/) that strikes that balance by splitting and storing small chunks of data. During retrieval, it first fetches the small chunks but then looks up the parent ids for those chunks and returns those larger documents.\n",
    "\n",
    "Note that \"parent document\" refers to the document that a small chunk originated from. This can either be the whole raw document OR a larger chunk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval\n",
    "\n",
    "RAPTOR enhances language models by organizing information into a hierarchical tree structure. It starts by dividing documents into small text chunks, which are then embedded, clustered based on similarity, and summarized. This process repeats recursively, creating multiple layers of summaries that capture information at varying levels of detail. When responding to queries, RAPTOR retrieves relevant information from different levels of this tree, allowing the model to integrate both detailed and high-level context. This method has demonstrated significant improvements in tasks requiring complex reasoning, achieving state-of-the-art results in benchmarks like QuALITY.\n",
    "\n",
    "![raptor](images/raptor.png)\n",
    "\n",
    "Links:\n",
    "\n",
    "- [RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval](https://arxiv.org/pdf/2401.18059) paper\n",
    "- [Deep dive video](https://www.youtube.com/watch?v=jbGchdTL7d0)\n",
    "- [Code repo](https://github.com/langchain-ai/langchain/blob/master/cookbook/RAPTOR.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the RAPTOR code repo as inspiration, lets obtain the LCEL docs to seed the database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raptor_load_docs():\n",
    "    # LCEL docs\n",
    "    url = \"https://python.langchain.com/docs/expression_language/\"\n",
    "    loader = RecursiveUrlLoader(\n",
    "        url=url, max_depth=20, extractor=lambda x: Soup(x, \"html.parser\").text\n",
    "    )\n",
    "    docs = loader.load()\n",
    "\n",
    "    # LCEL w/ PydanticOutputParser (outside the primary LCEL docs)\n",
    "    url = \"https://python.langchain.com/docs/how_to/output_parser_structured/\"\n",
    "    loader = RecursiveUrlLoader(\n",
    "        url=url, max_depth=1, extractor=lambda x: Soup(x, \"html.parser\").text\n",
    "    )\n",
    "    docs_pydantic = loader.load()\n",
    "\n",
    "    # LCEL w/ Self Query (outside the primary LCEL docs)\n",
    "    url = \"https://python.langchain.com/docs/how_to/self_query/\"\n",
    "    loader = RecursiveUrlLoader(\n",
    "        url=url, max_depth=1, extractor=lambda x: Soup(x, \"html.parser\").text\n",
    "    )\n",
    "    docs_sq = loader.load()\n",
    "\n",
    "    # Doc texts\n",
    "    docs.extend([*docs_pydantic, *docs_sq])\n",
    "    docs_texts = [d.page_content for d in docs]\n",
    "    \n",
    "    # Doc texts concat\n",
    "    docs_sorted = sorted(docs, key=lambda x: x.metadata[\"source\"])\n",
    "    docs_reversed = list(reversed(docs_sorted))\n",
    "    concatenated_content = \"\\n\\n\\n --- \\n\\n\\n\".join(\n",
    "        [doc.page_content for doc in docs_reversed]\n",
    "    )\n",
    "    print(\n",
    "        \"Num tokens in all context: %s\"\n",
    "        % num_tokens_from_string(concatenated_content, \"cl100k_base\")\n",
    "    )\n",
    "    \n",
    "    chunk_size_tok = 2000\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=chunk_size_tok, chunk_overlap=0\n",
    "    )\n",
    "    texts_split = text_splitter.split_text(concatenated_content)\n",
    "    return (docs_texts, texts_split)\n",
    "    \n",
    "raptor_load_docs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tree Construction\n",
    "\n",
    "The clustering approach in tree construction includes a few interesting ideas.\n",
    "\n",
    "**GMM (Gaussian Mixture Model)**\n",
    "\n",
    "- Model the distribution of data points across different clusters\n",
    "- Optimal number of clusters by evaluating the model's Bayesian Information Criterion (BIC)\n",
    "\n",
    "**UMAP (Uniform Manifold Approximation and Projection)**\n",
    "\n",
    "- Supports clustering\n",
    "- Reduces the dimensionality of high-dimensional data\n",
    "- UMAP helps to highlight the natural grouping of data points based on their similarities\n",
    "\n",
    "**Local and Global Clustering**\n",
    "\n",
    "- Used to analyze data at different scales\n",
    "- Both fine-grained and broader patterns within the data are captured effectively\n",
    "\n",
    "**Thresholding**\n",
    "\n",
    "- Apply in the context of GMM to determine cluster membership\n",
    "- Based on the probability distribution (assignment of data points to ≥ 1 cluster)\n",
    "\n",
    "Let's define the functions ([original source](https://github.com/parthsarthi03/raptor/blob/master/raptor/cluster_tree_builder.py)) we will need to build the tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 224  # Fixed seed for reproducibility\n",
    "\n",
    "def global_cluster_embeddings(\n",
    "    embeddings: np.ndarray,\n",
    "    dim: int,\n",
    "    n_neighbors: Optional[int] = None,\n",
    "    metric: str = \"cosine\",\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Perform global dimensionality reduction on the embeddings using UMAP.\n",
    "\n",
    "    Parameters:\n",
    "    - embeddings: The input embeddings as a numpy array.\n",
    "    - dim: The target dimensionality for the reduced space.\n",
    "    - n_neighbors: Optional; the number of neighbors to consider for each point.\n",
    "                   If not provided, it defaults to the square root of the number of embeddings.\n",
    "    - metric: The distance metric to use for UMAP.\n",
    "\n",
    "    Returns:\n",
    "    - A numpy array of the embeddings reduced to the specified dimensionality.\n",
    "    \"\"\"\n",
    "    if n_neighbors is None:\n",
    "        n_neighbors = int((len(embeddings) - 1) ** 0.5)\n",
    "    return umap.UMAP(\n",
    "        n_neighbors=n_neighbors, n_components=dim, metric=metric\n",
    "    ).fit_transform(embeddings)\n",
    "\n",
    "\n",
    "def local_cluster_embeddings(\n",
    "    embeddings: np.ndarray, dim: int, num_neighbors: int = 10, metric: str = \"cosine\"\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Perform local dimensionality reduction on the embeddings using UMAP, typically after global clustering.\n",
    "\n",
    "    Parameters:\n",
    "    - embeddings: The input embeddings as a numpy array.\n",
    "    - dim: The target dimensionality for the reduced space.\n",
    "    - num_neighbors: The number of neighbors to consider for each point.\n",
    "    - metric: The distance metric to use for UMAP.\n",
    "\n",
    "    Returns:\n",
    "    - A numpy array of the embeddings reduced to the specified dimensionality.\n",
    "    \"\"\"\n",
    "    return umap.UMAP(\n",
    "        n_neighbors=num_neighbors, n_components=dim, metric=metric\n",
    "    ).fit_transform(embeddings)\n",
    "\n",
    "\n",
    "def get_optimal_clusters(\n",
    "    embeddings: np.ndarray, max_clusters: int = 50, random_state: int = RANDOM_SEED\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Determine the optimal number of clusters using the Bayesian Information Criterion (BIC) with a Gaussian Mixture Model.\n",
    "\n",
    "    Parameters:\n",
    "    - embeddings: The input embeddings as a numpy array.\n",
    "    - max_clusters: The maximum number of clusters to consider.\n",
    "    - random_state: Seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    - An integer representing the optimal number of clusters found.\n",
    "    \"\"\"\n",
    "    max_clusters = min(max_clusters, len(embeddings))\n",
    "    n_clusters = np.arange(1, max_clusters)\n",
    "    bics = []\n",
    "    for n in n_clusters:\n",
    "        gm = GaussianMixture(n_components=n, random_state=random_state)\n",
    "        gm.fit(embeddings)\n",
    "        bics.append(gm.bic(embeddings))\n",
    "    return n_clusters[np.argmin(bics)]\n",
    "\n",
    "\n",
    "def GMM_cluster(embeddings: np.ndarray, threshold: float, random_state: int = 0):\n",
    "    \"\"\"\n",
    "    Cluster embeddings using a Gaussian Mixture Model (GMM) based on a probability threshold.\n",
    "\n",
    "    Parameters:\n",
    "    - embeddings: The input embeddings as a numpy array.\n",
    "    - threshold: The probability threshold for assigning an embedding to a cluster.\n",
    "    - random_state: Seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    - A tuple containing the cluster labels and the number of clusters determined.\n",
    "    \"\"\"\n",
    "    n_clusters = get_optimal_clusters(embeddings)\n",
    "    gm = GaussianMixture(n_components=n_clusters, random_state=random_state)\n",
    "    gm.fit(embeddings)\n",
    "    probs = gm.predict_proba(embeddings)\n",
    "    labels = [np.where(prob > threshold)[0] for prob in probs]\n",
    "    return labels, n_clusters\n",
    "\n",
    "\n",
    "def perform_clustering(\n",
    "    embeddings: np.ndarray,\n",
    "    dim: int,\n",
    "    threshold: float,\n",
    ") -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Perform clustering on the embeddings by first reducing their dimensionality globally, then clustering\n",
    "    using a Gaussian Mixture Model, and finally performing local clustering within each global cluster.\n",
    "\n",
    "    Parameters:\n",
    "    - embeddings: The input embeddings as a numpy array.\n",
    "    - dim: The target dimensionality for UMAP reduction.\n",
    "    - threshold: The probability threshold for assigning an embedding to a cluster in GMM.\n",
    "\n",
    "    Returns:\n",
    "    - A list of numpy arrays, where each array contains the cluster IDs for each embedding.\n",
    "    \"\"\"\n",
    "    if len(embeddings) <= dim + 1:\n",
    "        # Avoid clustering when there's insufficient data\n",
    "        return [np.array([0]) for _ in range(len(embeddings))]\n",
    "\n",
    "    # Global dimensionality reduction\n",
    "    reduced_embeddings_global = global_cluster_embeddings(embeddings, dim)\n",
    "    # Global clustering\n",
    "    global_clusters, n_global_clusters = GMM_cluster(\n",
    "        reduced_embeddings_global, threshold\n",
    "    )\n",
    "\n",
    "    all_local_clusters = [np.array([]) for _ in range(len(embeddings))]\n",
    "    total_clusters = 0\n",
    "\n",
    "    # Iterate through each global cluster to perform local clustering\n",
    "    for i in range(n_global_clusters):\n",
    "        # Extract embeddings belonging to the current global cluster\n",
    "        global_cluster_embeddings_ = embeddings[\n",
    "            np.array([i in gc for gc in global_clusters])\n",
    "        ]\n",
    "\n",
    "        if len(global_cluster_embeddings_) == 0:\n",
    "            continue\n",
    "        if len(global_cluster_embeddings_) <= dim + 1:\n",
    "            # Handle small clusters with direct assignment\n",
    "            local_clusters = [np.array([0]) for _ in global_cluster_embeddings_]\n",
    "            n_local_clusters = 1\n",
    "        else:\n",
    "            # Local dimensionality reduction and clustering\n",
    "            reduced_embeddings_local = local_cluster_embeddings(\n",
    "                global_cluster_embeddings_, dim\n",
    "            )\n",
    "            local_clusters, n_local_clusters = GMM_cluster(\n",
    "                reduced_embeddings_local, threshold\n",
    "            )\n",
    "\n",
    "        # Assign local cluster IDs, adjusting for total clusters already processed\n",
    "        for j in range(n_local_clusters):\n",
    "            local_cluster_embeddings_ = global_cluster_embeddings_[\n",
    "                np.array([j in lc for lc in local_clusters])\n",
    "            ]\n",
    "            indices = np.where(\n",
    "                (embeddings == local_cluster_embeddings_[:, None]).all(-1)\n",
    "            )[1]\n",
    "            for idx in indices:\n",
    "                all_local_clusters[idx] = np.append(\n",
    "                    all_local_clusters[idx], j + total_clusters\n",
    "                )\n",
    "\n",
    "        total_clusters += n_local_clusters\n",
    "\n",
    "    return all_local_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define additional functions ([original source](https://github.com/langchain-ai/langchain/blob/master/cookbook/RAPTOR.ipynb)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(texts):\n",
    "    \"\"\"\n",
    "    Generate embeddings for a list of text documents.\n",
    "\n",
    "    This function assumes the existence of an `embd` object with a method `embed_documents`\n",
    "    that takes a list of texts and returns their embeddings.\n",
    "\n",
    "    Parameters:\n",
    "    - texts: List[str], a list of text documents to be embedded.\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: An array of embeddings for the given text documents.\n",
    "    \"\"\"\n",
    "    text_embeddings = embeddings.embed_documents(texts)\n",
    "    text_embeddings_np = np.array(text_embeddings)\n",
    "    return text_embeddings_np\n",
    "\n",
    "\n",
    "def embed_cluster_texts(texts):\n",
    "    \"\"\"\n",
    "    Embeds a list of texts and clusters them, returning a DataFrame with texts, their embeddings, and cluster labels.\n",
    "\n",
    "    This function combines embedding generation and clustering into a single step. It assumes the existence\n",
    "    of a previously defined `perform_clustering` function that performs clustering on the embeddings.\n",
    "\n",
    "    Parameters:\n",
    "    - texts: List[str], a list of text documents to be processed.\n",
    "\n",
    "    Returns:\n",
    "    - pandas.DataFrame: A DataFrame containing the original texts, their embeddings, and the assigned cluster labels.\n",
    "    \"\"\"\n",
    "    text_embeddings_np = embed(texts)  # Generate embeddings\n",
    "    cluster_labels = perform_clustering(\n",
    "        text_embeddings_np, 10, 0.1\n",
    "    )  # Perform clustering on the embeddings\n",
    "    df = pd.DataFrame()  # Initialize a DataFrame to store the results\n",
    "    df[\"text\"] = texts  # Store original texts\n",
    "    df[\"embd\"] = list(text_embeddings_np)  # Store embeddings as a list in the DataFrame\n",
    "    df[\"cluster\"] = cluster_labels  # Store cluster labels\n",
    "    return df\n",
    "\n",
    "\n",
    "def fmt_txt(df: pd.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Formats the text documents in a DataFrame into a single string.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing the 'text' column with text documents to format.\n",
    "\n",
    "    Returns:\n",
    "    - A single string where all text documents are joined by a specific delimiter.\n",
    "    \"\"\"\n",
    "    unique_txt = df[\"text\"].tolist()\n",
    "    return \"--- --- \\n --- --- \".join(unique_txt)\n",
    "\n",
    "\n",
    "def embed_cluster_summarize_texts(\n",
    "    texts: List[str], level: int\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Embeds, clusters, and summarizes a list of texts. This function first generates embeddings for the texts,\n",
    "    clusters them based on similarity, expands the cluster assignments for easier processing, and then summarizes\n",
    "    the content within each cluster.\n",
    "\n",
    "    Parameters:\n",
    "    - texts: A list of text documents to be processed.\n",
    "    - level: An integer parameter that could define the depth or detail of processing.\n",
    "\n",
    "    Returns:\n",
    "    - Tuple containing two DataFrames:\n",
    "      1. The first DataFrame (`df_clusters`) includes the original texts, their embeddings, and cluster assignments.\n",
    "      2. The second DataFrame (`df_summary`) contains summaries for each cluster, the specified level of detail,\n",
    "         and the cluster identifiers.\n",
    "    \"\"\"\n",
    "\n",
    "    # Embed and cluster the texts, resulting in a DataFrame with 'text', 'embd', and 'cluster' columns\n",
    "    df_clusters = embed_cluster_texts(texts)\n",
    "\n",
    "    # Prepare to expand the DataFrame for easier manipulation of clusters\n",
    "    expanded_list = []\n",
    "\n",
    "    # Expand DataFrame entries to document-cluster pairings for straightforward processing\n",
    "    for index, row in df_clusters.iterrows():\n",
    "        for cluster in row[\"cluster\"]:\n",
    "            expanded_list.append(\n",
    "                {\"text\": row[\"text\"], \"embd\": row[\"embd\"], \"cluster\": cluster}\n",
    "            )\n",
    "\n",
    "    # Create a new DataFrame from the expanded list\n",
    "    expanded_df = pd.DataFrame(expanded_list)\n",
    "\n",
    "    # Retrieve unique cluster identifiers for processing\n",
    "    all_clusters = expanded_df[\"cluster\"].unique()\n",
    "\n",
    "    print(f\"--Generated {len(all_clusters)} clusters--\")\n",
    "\n",
    "    # Summarization\n",
    "    template = \"\"\"Here is a sub-set of LangChain Expression Language doc. \n",
    "    \n",
    "    LangChain Expression Language provides a way to compose chain in LangChain.\n",
    "    \n",
    "    Give a detailed summary of the documentation provided.\n",
    "    \n",
    "    Documentation:\n",
    "    {context}\n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    chain = prompt | llm_claude_3_5_sonnet_v2 | StrOutputParser()\n",
    "\n",
    "    # Format text within each cluster for summarization\n",
    "    summaries = []\n",
    "    for i in all_clusters:\n",
    "        df_cluster = expanded_df[expanded_df[\"cluster\"] == i]\n",
    "        formatted_txt = fmt_txt(df_cluster)\n",
    "        summaries.append(chain.invoke({\"context\": formatted_txt}))\n",
    "\n",
    "    # Create a DataFrame to store summaries with their corresponding cluster and level\n",
    "    df_summary = pd.DataFrame(\n",
    "        {\n",
    "            \"summaries\": summaries,\n",
    "            \"level\": [level] * len(summaries),\n",
    "            \"cluster\": list(all_clusters),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return df_clusters, df_summary\n",
    "\n",
    "\n",
    "def recursive_embed_cluster_summarize(\n",
    "    texts: List[str], level: int = 1, n_levels: int = 3\n",
    ") -> Dict[int, Tuple[pd.DataFrame, pd.DataFrame]]:\n",
    "    \"\"\"\n",
    "    Recursively embeds, clusters, and summarizes texts up to a specified level or until\n",
    "    the number of unique clusters becomes 1, storing the results at each level.\n",
    "\n",
    "    Parameters:\n",
    "    - texts: List[str], texts to be processed.\n",
    "    - level: int, current recursion level (starts at 1).\n",
    "    - n_levels: int, maximum depth of recursion.\n",
    "\n",
    "    Returns:\n",
    "    - Dict[int, Tuple[pd.DataFrame, pd.DataFrame]], a dictionary where keys are the recursion\n",
    "      levels and values are tuples containing the clusters DataFrame and summaries DataFrame at that level.\n",
    "    \"\"\"\n",
    "    results = {}  # Dictionary to store results at each level\n",
    "\n",
    "    # Perform embedding, clustering, and summarization for the current level\n",
    "    df_clusters, df_summary = embed_cluster_summarize_texts(texts, level)\n",
    "\n",
    "    # Store the results of the current level\n",
    "    results[level] = (df_clusters, df_summary)\n",
    "\n",
    "    # Determine if further recursion is possible and meaningful\n",
    "    unique_clusters = df_summary[\"cluster\"].nunique()\n",
    "    if level < n_levels and unique_clusters > 1:\n",
    "        # Use summaries as the input texts for the next level of recursion\n",
    "        new_texts = df_summary[\"summaries\"].tolist()\n",
    "        next_level_results = recursive_embed_cluster_summarize(\n",
    "            new_texts, level + 1, n_levels\n",
    "        )\n",
    "\n",
    "        # Merge the results from the next level into the current results dictionary\n",
    "        results.update(next_level_results)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now build the tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(leaf_texts, texts_split) = raptor_load_docs()\n",
    "\n",
    "results = recursive_embed_cluster_summarize(leaf_texts, level=1, n_levels=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paper reports best performance from collapsed tree retrieval.\n",
    "\n",
    "This involves flattening the tree structure into a single layer and then applying a k-nearest neighbors (kNN) search across all nodes simultaneously.\n",
    "\n",
    "We do simply do this below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize all_texts with leaf_texts\n",
    "all_texts = leaf_texts.copy()\n",
    "\n",
    "# Iterate through the results to extract summaries from each level and add them to all_texts\n",
    "for level in sorted(results.keys()):\n",
    "    # Extract summaries from the current level's DataFrame\n",
    "    summaries = results[level][1][\"summaries\"].tolist()\n",
    "    # Extend all_texts with the summaries from the current level\n",
    "    all_texts.extend(summaries)\n",
    "\n",
    "# Now, use all_texts to build the vectorstore with Chroma\n",
    "vectorstore_raptor = Chroma.from_texts(texts=all_texts, embedding=embeddings)\n",
    "retriever_raptor = vectorstore_raptor.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can using our flattened, indexed tree in a RAG chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "prompt_raptor = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# Post-processing\n",
    "def format_docs_raptor(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "# Chain\n",
    "chain_raptor = (\n",
    "    {\"context\": retriever | format_docs_raptor, \"question\": RunnablePassthrough()}\n",
    "    | prompt_raptor\n",
    "    | llm_claude_3_5_sonnet_v2\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Question\n",
    "chain_raptor.invoke(\"How to define a RAG chain? Give me a specific code example.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ColBERT: Contextualized Late Interaction over BERT\n",
    "\n",
    "> **Note:** Attempting to load the pretrained ColBERT model is failing on my Mac M1 with `RuntimeError: Error building extension 'segmented_maxsim_cpp'`. Have not been able to find a fix for this yet, therefore the code in this section may fail.\n",
    "\n",
    "ColBERT is a neural retrieval model that enhances search efficiency and effectiveness by combining deep language model representations with scalable retrieval techniques. Unlike traditional models that encode queries and documents into single vectors, ColBERT generates multiple vectors at the token level, allowing for fine-grained interactions between queries and documents. This late interaction mechanism enables ColBERT to pre-compute document representations offline, significantly speeding up query processing. By leveraging vector-similarity search indexes, ColBERT can perform end-to-end retrieval directly from large document collections, achieving high recall and precision while maintaining computational efficiency. \n",
    "\n",
    "To explain more clearly: You embed the query and the passage and get vector representation for every token in both. Then, for each query token, you find the token in the passage with the largest dot product (i.e. the largest similarity). This is called the “maxsim” for each token. Finally, the similarity score between the query and the passage is the summation of all the maxsims you just found\n",
    "\n",
    "The [RAGatouille](https://python.langchain.com/docs/integrations/retrievers/ragatouille/) library includes support for ColBERT.\n",
    "\n",
    "\n",
    "Links:\n",
    "\n",
    "- [ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction)](https://arxiv.org/pdf/2112.01488) paper\n",
    "\n",
    "To create an index, you'll need to load a trained model, this can be one of your own or a pretrained one from the hub! Creating an index with the default configuration is just a few lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dec 28, 16:20:04] Loading segmented_maxsim_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /Users/deanhart/Library/Caches/torch_extensions/py311_cpu as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module segmented_maxsim_cpp, skipping build step...\n",
      "Loading extension module segmented_maxsim_cpp...\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "dlopen(/Users/deanhart/Library/Caches/torch_extensions/py311_cpu/segmented_maxsim_cpp/segmented_maxsim_cpp.so, 0x0002): tried: '/Users/deanhart/Library/Caches/torch_extensions/py311_cpu/segmented_maxsim_cpp/segmented_maxsim_cpp.so' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/Users/deanhart/Library/Caches/torch_extensions/py311_cpu/segmented_maxsim_cpp/segmented_maxsim_cpp.so' (no such file), '/Users/deanhart/Library/Caches/torch_extensions/py311_cpu/segmented_maxsim_cpp/segmented_maxsim_cpp.so' (no such file)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m rag_colbert \u001b[38;5;241m=\u001b[39m RAGPretrainedModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolbert-ir/colbertv2.0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m docs_colbert \u001b[38;5;241m=\u001b[39m [get_wikipedia_page(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHayao_Miyazaki\u001b[39m\u001b[38;5;124m\"\u001b[39m), get_wikipedia_page(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStudio_Ghibli\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[1;32m      3\u001b[0m colbert_index_path \u001b[38;5;241m=\u001b[39m rag_colbert\u001b[38;5;241m.\u001b[39mindex(index_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmy_index\u001b[39m\u001b[38;5;124m\"\u001b[39m, collection\u001b[38;5;241m=\u001b[39mdocs_colbert)\n",
      "File \u001b[0;32m~/miniconda3/envs/rag-techniques/lib/python3.11/site-packages/ragatouille/RAGPretrainedModel.py:71\u001b[0m, in \u001b[0;36mRAGPretrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, n_gpu, verbose, index_root)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load a ColBERT model from a pre-trained checkpoint.\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \n\u001b[1;32m     61\u001b[0m \u001b[38;5;124;03mParameters:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;124;03m    cls (RAGPretrainedModel): The current instance of RAGPretrainedModel, with the model initialised.\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     70\u001b[0m instance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m()\n\u001b[0;32m---> 71\u001b[0m instance\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m ColBERT(\n\u001b[1;32m     72\u001b[0m     pretrained_model_name_or_path, n_gpu, index_root\u001b[38;5;241m=\u001b[39mindex_root, verbose\u001b[38;5;241m=\u001b[39mverbose\n\u001b[1;32m     73\u001b[0m )\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m instance\n",
      "File \u001b[0;32m~/miniconda3/envs/rag-techniques/lib/python3.11/site-packages/ragatouille/models/colbert.py:84\u001b[0m, in \u001b[0;36mColBERT.__init__\u001b[0;34m(self, pretrained_model_name_or_path, n_gpu, index_name, verbose, load_from_index, training_mode, index_root, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mroot \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex_root\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m training_mode:\n\u001b[0;32m---> 84\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minference_ckpt \u001b[38;5;241m=\u001b[39m Checkpoint(\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheckpoint, colbert_config\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\n\u001b[1;32m     86\u001b[0m     )\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model_max_tokens \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     88\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minference_ckpt\u001b[38;5;241m.\u001b[39mbert\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmax_position_embeddings\n\u001b[1;32m     89\u001b[0m     ) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_context \u001b[38;5;241m=\u001b[39m Run()\u001b[38;5;241m.\u001b[39mcontext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_config)\n",
      "File \u001b[0;32m~/miniconda3/envs/rag-techniques/lib/python3.11/site-packages/colbert/modeling/checkpoint.py:19\u001b[0m, in \u001b[0;36mCheckpoint.__init__\u001b[0;34m(self, name, colbert_config, verbose)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, colbert_config\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, verbose:\u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m):\n\u001b[0;32m---> 19\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(name, colbert_config)\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m=\u001b[39m verbose\n",
      "File \u001b[0;32m~/miniconda3/envs/rag-techniques/lib/python3.11/site-packages/colbert/modeling/colbert.py:24\u001b[0m, in \u001b[0;36mColBERT.__init__\u001b[0;34m(self, name, colbert_config)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(name, colbert_config)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_gpu \u001b[38;5;241m=\u001b[39m colbert_config\u001b[38;5;241m.\u001b[39mtotal_visible_gpus \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 24\u001b[0m ColBERT\u001b[38;5;241m.\u001b[39mtry_load_torch_extensions(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_gpu)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolbert_config\u001b[38;5;241m.\u001b[39mmask_punctuation:\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskiplist \u001b[38;5;241m=\u001b[39m {w: \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     28\u001b[0m                      \u001b[38;5;28;01mfor\u001b[39;00m symbol \u001b[38;5;129;01min\u001b[39;00m string\u001b[38;5;241m.\u001b[39mpunctuation\n\u001b[1;32m     29\u001b[0m                      \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m [symbol, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_tokenizer\u001b[38;5;241m.\u001b[39mencode(symbol, add_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]]}\n",
      "File \u001b[0;32m~/miniconda3/envs/rag-techniques/lib/python3.11/site-packages/colbert/modeling/colbert.py:39\u001b[0m, in \u001b[0;36mColBERT.try_load_torch_extensions\u001b[0;34m(cls, use_gpu)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     38\u001b[0m print_message(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading segmented_maxsim_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 39\u001b[0m segmented_maxsim_cpp \u001b[38;5;241m=\u001b[39m load(\n\u001b[1;32m     40\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msegmented_maxsim_cpp\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     41\u001b[0m     sources\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     42\u001b[0m         os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m     43\u001b[0m             pathlib\u001b[38;5;241m.\u001b[39mPath(\u001b[38;5;18m__file__\u001b[39m)\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mresolve(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msegmented_maxsim.cpp\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     44\u001b[0m         ),\n\u001b[1;32m     45\u001b[0m     ],\n\u001b[1;32m     46\u001b[0m     extra_cflags\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-O3\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     47\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCOLBERT_LOAD_TORCH_EXTENSION_VERBOSE\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFalse\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrue\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     48\u001b[0m )\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39msegmented_maxsim \u001b[38;5;241m=\u001b[39m segmented_maxsim_cpp\u001b[38;5;241m.\u001b[39msegmented_maxsim_cpp\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mloaded_extensions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rag-techniques/lib/python3.11/site-packages/torch/utils/cpp_extension.py:1314\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, sources, extra_cflags, extra_cuda_cflags, extra_ldflags, extra_include_paths, build_directory, verbose, with_cuda, is_python_module, is_standalone, keep_intermediates)\u001b[0m\n\u001b[1;32m   1222\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(name,\n\u001b[1;32m   1223\u001b[0m          sources: Union[\u001b[38;5;28mstr\u001b[39m, List[\u001b[38;5;28mstr\u001b[39m]],\n\u001b[1;32m   1224\u001b[0m          extra_cflags\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1232\u001b[0m          is_standalone\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1233\u001b[0m          keep_intermediates\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m   1234\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1235\u001b[0m \u001b[38;5;124;03m    Load a PyTorch C++ extension just-in-time (JIT).\u001b[39;00m\n\u001b[1;32m   1236\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;124;03m        ...     verbose=True)\u001b[39;00m\n\u001b[1;32m   1313\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _jit_compile(\n\u001b[1;32m   1315\u001b[0m         name,\n\u001b[1;32m   1316\u001b[0m         [sources] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(sources, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m sources,\n\u001b[1;32m   1317\u001b[0m         extra_cflags,\n\u001b[1;32m   1318\u001b[0m         extra_cuda_cflags,\n\u001b[1;32m   1319\u001b[0m         extra_ldflags,\n\u001b[1;32m   1320\u001b[0m         extra_include_paths,\n\u001b[1;32m   1321\u001b[0m         build_directory \u001b[38;5;129;01mor\u001b[39;00m _get_build_directory(name, verbose),\n\u001b[1;32m   1322\u001b[0m         verbose,\n\u001b[1;32m   1323\u001b[0m         with_cuda,\n\u001b[1;32m   1324\u001b[0m         is_python_module,\n\u001b[1;32m   1325\u001b[0m         is_standalone,\n\u001b[1;32m   1326\u001b[0m         keep_intermediates\u001b[38;5;241m=\u001b[39mkeep_intermediates)\n",
      "File \u001b[0;32m~/miniconda3/envs/rag-techniques/lib/python3.11/site-packages/torch/utils/cpp_extension.py:1746\u001b[0m, in \u001b[0;36m_jit_compile\u001b[0;34m(name, sources, extra_cflags, extra_cuda_cflags, extra_ldflags, extra_include_paths, build_directory, verbose, with_cuda, is_python_module, is_standalone, keep_intermediates)\u001b[0m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_standalone:\n\u001b[1;32m   1744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _get_exec_path(name, build_directory)\n\u001b[0;32m-> 1746\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _import_module_from_library(name, build_directory, is_python_module)\n",
      "File \u001b[0;32m~/miniconda3/envs/rag-techniques/lib/python3.11/site-packages/torch/utils/cpp_extension.py:2140\u001b[0m, in \u001b[0;36m_import_module_from_library\u001b[0;34m(module_name, path, is_python_module)\u001b[0m\n\u001b[1;32m   2138\u001b[0m spec \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mutil\u001b[38;5;241m.\u001b[39mspec_from_file_location(module_name, filepath)\n\u001b[1;32m   2139\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2140\u001b[0m module \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mutil\u001b[38;5;241m.\u001b[39mmodule_from_spec(spec)\n\u001b[1;32m   2141\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(spec\u001b[38;5;241m.\u001b[39mloader, importlib\u001b[38;5;241m.\u001b[39mabc\u001b[38;5;241m.\u001b[39mLoader)\n\u001b[1;32m   2142\u001b[0m spec\u001b[38;5;241m.\u001b[39mloader\u001b[38;5;241m.\u001b[39mexec_module(module)\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:573\u001b[0m, in \u001b[0;36mmodule_from_spec\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1233\u001b[0m, in \u001b[0;36mcreate_module\u001b[0;34m(self, spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: dlopen(/Users/deanhart/Library/Caches/torch_extensions/py311_cpu/segmented_maxsim_cpp/segmented_maxsim_cpp.so, 0x0002): tried: '/Users/deanhart/Library/Caches/torch_extensions/py311_cpu/segmented_maxsim_cpp/segmented_maxsim_cpp.so' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/Users/deanhart/Library/Caches/torch_extensions/py311_cpu/segmented_maxsim_cpp/segmented_maxsim_cpp.so' (no such file), '/Users/deanhart/Library/Caches/torch_extensions/py311_cpu/segmented_maxsim_cpp/segmented_maxsim_cpp.so' (no such file)"
     ]
    }
   ],
   "source": [
    "rag_colbert = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
    "docs_colbert = [get_wikipedia_page(\"Hayao_Miyazaki\"), get_wikipedia_page(\"Studio_Ghibli\")]\n",
    "colbert_index_path = rag_colbert.index(\n",
    "    collection=docs_colbert,\n",
    "    index_name=\"my_index\", \n",
    "    max_document_length=180,\n",
    "    split_documents=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also optionally add document IDs or document metadata when creating the index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rag_colbert' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m doc_ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmiyazaki\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mghibli\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      2\u001b[0m doc_metadatas \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      3\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mentity\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mperson\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwikipedia\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m      4\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mentity\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morganisation\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwikipedia\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m      5\u001b[0m ]\n\u001b[0;32m----> 6\u001b[0m colbert_index_path \u001b[38;5;241m=\u001b[39m rag_colbert\u001b[38;5;241m.\u001b[39mindex(\n\u001b[1;32m      7\u001b[0m     index_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmy_index_with_ids_and_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      8\u001b[0m     collection\u001b[38;5;241m=\u001b[39mdocs_colbert,\n\u001b[1;32m      9\u001b[0m     document_ids\u001b[38;5;241m=\u001b[39mdoc_ids,\n\u001b[1;32m     10\u001b[0m     document_metadatas\u001b[38;5;241m=\u001b[39mdoc_metadatas,\n\u001b[1;32m     11\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rag_colbert' is not defined"
     ]
    }
   ],
   "source": [
    "doc_ids = [\"miyazaki\", \"ghibli\"]\n",
    "doc_metadatas = [\n",
    "    {\"entity\": \"person\", \"source\": \"wikipedia\"},\n",
    "    {\"entity\": \"organisation\", \"source\": \"wikipedia\"},\n",
    "]\n",
    "colbert_index_path = rag_colbert.index(\n",
    "    index_name=\"my_index_with_ids_and_metadata\",\n",
    "    collection=docs_colbert,\n",
    "    document_ids=doc_ids,\n",
    "    document_metadatas=doc_metadatas,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once this is done running, your index will be saved on-disk and ready to be queried! RAGatouille and ColBERT handle everything here:\n",
    "\n",
    "- Splitting your documents\n",
    "- Tokenizing your documents\n",
    "- Identifying the individual terms\n",
    "- Embedding the documents and generating the bags-of-embeddings\n",
    "- Compressing the vectors and storing them on disk\n",
    "\n",
    "Once an index is created, querying it is just as simple as creating it! You can either load the model you need directly from an index's configuration:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = rag_colbert.search(query=\"What animation studio did Miyazaki found?\", k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAG.search is a flexible method! You can set the k value to however many results you want (it defaults to 10), and you can also use it to search for multiple queries at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_colbert.search([\"What manga did Hayao Miyazaki write?\",\n",
    "                    \"Who are the founders of Ghibli?\"\n",
    "                    \"Who is the director of Spirited Away?\"],)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also search using a retriever:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_colbert = rag_colbert.as_langchain_retriever(k=3)\n",
    "retriever_colbert.invoke(\"What animation studio did Miyazaki found?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval\n",
    "\n",
    "### Re-ranking\n",
    "\n",
    "![re-ranking](images/re-ranking.png)\n",
    "\n",
    "We touched on this subject earlier when we looked at RAG-Fusion where we implemented a recipricol rank fusion function to re-rank results. But lets look at how to achieve the same using the recently released _rerank_ model from Amazon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-techniques",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
