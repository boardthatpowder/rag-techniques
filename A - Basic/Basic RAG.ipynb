{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic RAG\n",
    "\n",
    "![basic rag](../images/images-basic%20rag.png)\n",
    "\n",
    "A basic **Retrieval-Augmented Generation (RAG)** flow can be broken down into three main stages: **Indexing**, **Retrieval**, and **Generation**. Each stage plays a distinct role in ensuring that the user query is processed effectively and results in a relevant, contextually enriched response.\n",
    "\n",
    "### **1. Indexing**\n",
    "   - **Purpose**: Prepare the external knowledge base for efficient retrieval.\n",
    "   - **Process**:\n",
    "     1. **Document Preprocessing**:\n",
    "        - Raw data (e.g., PDFs, web pages, structured data) is cleaned, tokenized, and converted into a machine-readable format.\n",
    "     2. **Representation**:\n",
    "        - Documents are encoded into a searchable form:\n",
    "          - **Sparse Indexing**: Traditional techniques like TF-IDF or BM25 create keyword-based indexes.\n",
    "          - **Dense Indexing**: Neural models (e.g., Sentence Transformers) create vector embeddings for semantic similarity search.\n",
    "     3. **Storage**:\n",
    "        - The indexed representations are stored in a retrieval system like Chroma, Elasticsearch, FAISS, Pinecone, or Vespa.\n",
    "   - **Output**: A structured, searchable repository of documents ready for retrieval.\n",
    "\n",
    "### **2. Retrieval**\n",
    "   - **Purpose**: Identify and fetch relevant documents or passages from the knowledge base.\n",
    "   - **Process**:\n",
    "     1. **Query Encoding**:\n",
    "        - The user query is encoded into a vector representation (using the same embedding model as the indexing step for dense retrieval).\n",
    "     2. **Search**:\n",
    "        - The encoded query is matched against the indexed documents:\n",
    "          - **Dense Retrieval**: Measures similarity (e.g., cosine similarity) between the query embedding and document embeddings.\n",
    "          - **Sparse Retrieval**: Uses keyword-based scoring algorithms like BM25.\n",
    "     3. **Ranking**:\n",
    "        - Retrieved documents are ranked based on relevance scores.\n",
    "     4. **Selection**:\n",
    "        - A fixed number (e.g., top 5) of the most relevant documents or passages are selected.\n",
    "   - **Output**: A set of top-ranked documents or snippets that are most relevant to the query.\n",
    "\n",
    "### **3. Generation**\n",
    "   - **Purpose**: Use retrieved information to generate a coherent and contextually accurate response.\n",
    "   - **Process**:\n",
    "     1. **Context Preparation**:\n",
    "        - The user query and the retrieved documents are combined into a prompt for the LLM.\n",
    "     2. **Language Model Processing**:\n",
    "        - The LLM processes the input, paying attention to the query and retrieved context to craft a grounded response.\n",
    "     3. **Response Optimization**:\n",
    "        - The output may be fine-tuned to ensure clarity, coherence, and relevance (e.g., using post-processing techniques).\n",
    "   - **Output**: A natural language response tailored to the user's query, enriched with the retrieved contextual information.\n",
    "\n",
    "### **Example Flow**\n",
    "#### User Query:\n",
    "*\"What are the best practices for securing an API?\"*\n",
    "\n",
    "1. **Indexing**:\n",
    "   - Security guidelines, blog posts, and API documentation are preprocessed and indexed using dense embeddings and stored in a vector database.\n",
    "\n",
    "2. **Retrieval**:\n",
    "   - The query is encoded into a vector and matched against the database.\n",
    "   - Relevant documents such as \"API Security Best Practices (2023)\" and \"OAuth Implementation Guide\" are retrieved.\n",
    "\n",
    "3. **Generation**:\n",
    "   - The query and retrieved documents are passed as input to an LLM.\n",
    "   - The model generates a response like:\n",
    "     - \"To secure an API, implement OAuth 2.0 for authentication, validate all inputs to prevent injection attacks, and ensure HTTPS is enforced for all connections.\"\n",
    "\n",
    "### **Key Points**\n",
    "- **Indexing** ensures efficient retrieval by pre-processing and storing documents in a searchable format.\n",
    "- **Retrieval** narrows down the knowledge base to the most relevant context.\n",
    "- **Generation** synthesizes this context with the user query to produce an accurate, grounded response.\n",
    "\n",
    "This modular flow allows RAG systems to dynamically incorporate external information, making them versatile and scalable for diverse use cases.\n",
    "\n",
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"../Z - Common/setup.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.globals import set_debug\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from pprint import pprint\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing\n",
    "\n",
    "[DocumentLoaders](https://python.langchain.com/docs/integrations/document_loaders/) load data into the standard LangChain Document format. Each DocumentLoader has its own specific parameters, but they can all be invoked in the same way with the `.load()` method. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;32mdef\u001b[0m \u001b[0mload_sample_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mIterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDocument\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Loads data from a blog, intended to be later stored in a vectorstore.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWebBaseLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mweb_paths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://lilianweng.github.io/posts/2023-06-23-agent/\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mbs_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mparse_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbs4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSoupStrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mclass_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"post-content\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"post-title\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"post-header\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%psource load_sample_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = load_sample_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before loading the documents into vector store they need to be [split](https://python.langchain.com/docs/how_to/recursive_text_splitter/). Here we are starting off with basic splitting by length (`300` characters with `50` overlap), but later will explore other splitting techniques.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;32mdef\u001b[0m \u001b[0msplit_sample_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDocument\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_overlap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDocument\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtext_splitter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRecursiveCharacterTextSplitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tiktoken_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mchunk_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mchunk_overlap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_overlap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# Make splits\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0msplits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_splitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0msplits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%psource split_sample_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = split_sample_data(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then finally we can load the split embeddings into the [vector store](https://python.langchain.com/docs/integrations/vectorstores/). For simplicity we are using [Chroma](https://python.langchain.com/docs/integrations/vectorstores/chroma/) as the vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;32mdef\u001b[0m \u001b[0mseed_sample_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDocument\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mVectorStoreRetriever\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mvector_store\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mChroma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mcollection_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"rag_techniques\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0membedding_function\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mpersist_directory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"./chroma_db\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0muuids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muuid4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mvector_store\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muuids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mretriever\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvector_store\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_retriever\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msearch_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"k\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mretriever\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%psource seed_sample_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = seed_sample_data(splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval\n",
    "\n",
    "Using the `retriever` we can query the vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of results:  1\n",
      "{'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "Fig. 1. Overview of a LLM-powered autonomous agent system.\n",
      "Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
      "Task Decomposition#\n",
      "Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\n",
      "Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
      "Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n"
     ]
    }
   ],
   "source": [
    "docs = retriever.invoke(\"What is Task Decomposition?\")\n",
    "\n",
    "print(\"No. of results: \", len(docs))\n",
    "print(docs[0].metadata)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation\n",
    "\n",
    "Define the prompt we will pass to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='Answer the question based only on the following context:\\n{context}\\n\\nQuestion: {question}\\n'), additional_kwargs={})])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build  a basic chain. The `|` operator [chains runnable objects](https://python.langchain.com/docs/how_to/sequence/) (objects that have an `invoke()` function) together so as one object is streaming output, the next object in the chain can receive the stream as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the chain by calling its invoke method. The `dict` passed to `invoke()` is used to tokenize varibles declared at any of the steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Based on the provided context, Task Decomposition is a process where complex tasks are broken down into smaller, more manageable steps. It can be accomplished through several methods:\\n\\n1. Chain of thought (CoT) - a prompting technique where the model is instructed to \"think step by step\" to break down complex tasks into simpler steps.\\n\\n2. Tree of Thoughts - an extension of CoT that explores multiple reasoning possibilities at each step, creating a tree structure that can be searched using BFS or DFS.\\n\\nTask decomposition can be implemented in three ways:\\n1. Using LLM with simple prompts (e.g., \"Steps for XYZ.\\\\n1.\" or \"What are the subgoals for achieving XYZ?\")\\n2. Using task-specific instructions (e.g., \"Write a story outline\" for writing a novel)\\n3. With human inputs\\n\\nThe purpose of task decomposition is to make complicated tasks more manageable and provide insight into the model\\'s thinking process.', additional_kwargs={'usage': {'prompt_tokens': 371, 'completion_tokens': 220, 'total_tokens': 591}, 'stop_reason': 'end_turn', 'model_id': 'anthropic.claude-3-5-sonnet-20241022-v2:0'}, response_metadata={'usage': {'prompt_tokens': 371, 'completion_tokens': 220, 'total_tokens': 591}, 'stop_reason': 'end_turn', 'model_id': 'anthropic.claude-3-5-sonnet-20241022-v2:0'}, id='run-7b1dd3f4-98ec-4d51-8047-590d8f433ee7-0', usage_metadata={'input_tokens': 371, 'output_tokens': 220, 'total_tokens': 591})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"context\":docs,\"question\":\"What is Task Decomposition?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of defining our own prompts, we can make use of prompt templates published in the [Langchain Hub](https://smith.langchain.com/hub). Lets replace our previous prompt with one from the hub and rebuild the chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"), additional_kwargs={})])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Task Decomposition is a technique where complex tasks are broken down into smaller, more manageable steps, often implemented through methods like Chain of Thought (CoT) prompting. It can be accomplished through LLM prompting, task-specific instructions, or human inputs, and helps make complicated tasks more approachable while providing insight into the model's thinking process. Advanced versions like Tree of Thoughts extend this concept by exploring multiple reasoning possibilities at each step.\", additional_kwargs={'usage': {'prompt_tokens': 418, 'completion_tokens': 98, 'total_tokens': 516}, 'stop_reason': 'end_turn', 'model_id': 'anthropic.claude-3-5-sonnet-20241022-v2:0'}, response_metadata={'usage': {'prompt_tokens': 418, 'completion_tokens': 98, 'total_tokens': 516}, 'stop_reason': 'end_turn', 'model_id': 'anthropic.claude-3-5-sonnet-20241022-v2:0'}, id='run-e33b773e-fb2c-4c6b-b0aa-371ef4eccb21-0', usage_metadata={'input_tokens': 418, 'output_tokens': 98, 'total_tokens': 516})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "pprint(prompt)\n",
    "\n",
    "# rebuild the chain\n",
    "chain = prompt | llm\n",
    "\n",
    "chain.invoke({\"context\":docs, \"question\":\"What is Task Decomposition?\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now build a basic RAG chain, where instead of explicitly passing `docs` as the context we instead provide the `retriever` to query the vector store directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task Decomposition is a technique where complex tasks are broken down into smaller, more manageable steps, often implemented through methods like Chain of Thought (CoT) prompting. It can be accomplished through LLM prompting, task-specific instructions, or human inputs, and helps models tackle complicated problems more effectively. Advanced versions like Tree of Thoughts extend this concept by exploring multiple reasoning possibilities at each step.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "result = chain.invoke(\"What is Task Decomposition?\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are interested in understanding more details about the chain, we can run it in debug mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What are the main components of an LLM-powered autonomous agent system?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,question>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What are the main components of an LLM-powered autonomous agent system?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,question> > chain:RunnablePassthrough] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What are the main components of an LLM-powered autonomous agent system?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,question> > chain:RunnablePassthrough] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"What are the main components of an LLM-powered autonomous agent system?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,question>] [301ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] [2ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatBedrock] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: What are the main components of an LLM-powered autonomous agent system? \\nContext: [Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='LLM Powered Autonomous Agents\\\\n    \\\\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\\\n\\\\n\\\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\\\nAgent System Overview#\\\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\\\n\\\\nPlanning\\\\n\\\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\\\n\\\\n\\\\nMemory')] \\nAnswer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatBedrock] [2.86s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"Based on the context, the main components of an LLM-powered autonomous agent system include the LLM itself functioning as the brain, a planning component (which handles subgoal decomposition and task breakdown), and a memory component. The system also includes reflection and refinement capabilities, allowing the agent to learn from past actions and improve through self-criticism.\",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"Based on the context, the main components of an LLM-powered autonomous agent system include the LLM itself functioning as the brain, a planning component (which handles subgoal decomposition and task breakdown), and a memory component. The system also includes reflection and refinement capabilities, allowing the agent to learn from past actions and improve through self-criticism.\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"usage\": {\n",
      "                \"prompt_tokens\": 383,\n",
      "                \"completion_tokens\": 77,\n",
      "                \"total_tokens\": 460\n",
      "              },\n",
      "              \"stop_reason\": \"end_turn\",\n",
      "              \"model_id\": \"anthropic.claude-3-5-sonnet-20241022-v2:0\"\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"usage\": {\n",
      "                \"prompt_tokens\": 383,\n",
      "                \"completion_tokens\": 77,\n",
      "                \"total_tokens\": 460\n",
      "              },\n",
      "              \"stop_reason\": \"end_turn\",\n",
      "              \"model_id\": \"anthropic.claude-3-5-sonnet-20241022-v2:0\"\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-e2c31c98-62ab-4aa2-98e1-52d1d6eeb751-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 383,\n",
      "              \"output_tokens\": 77,\n",
      "              \"total_tokens\": 460\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"usage\": {\n",
      "      \"prompt_tokens\": 383,\n",
      "      \"completion_tokens\": 77,\n",
      "      \"total_tokens\": 460\n",
      "    },\n",
      "    \"stop_reason\": \"end_turn\",\n",
      "    \"model_id\": \"anthropic.claude-3-5-sonnet-20241022-v2:0\"\n",
      "  },\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:StrOutputParser] [1ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"Based on the context, the main components of an LLM-powered autonomous agent system include the LLM itself functioning as the brain, a planning component (which handles subgoal decomposition and task breakdown), and a memory component. The system also includes reflection and refinement capabilities, allowing the agent to learn from past actions and improve through self-criticism.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [3.18s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"Based on the context, the main components of an LLM-powered autonomous agent system include the LLM itself functioning as the brain, a planning component (which handles subgoal decomposition and task breakdown), and a memory component. The system also includes reflection and refinement capabilities, allowing the agent to learn from past actions and improve through self-criticism.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "set_debug(True)\n",
    "\n",
    "result = chain.invoke(\"What are the main components of an LLM-powered autonomous agent system?\")\n",
    "result\n",
    "\n",
    "set_debug(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Langsmith](https://smith.langchain.com/) also allows you to view the flow. To view:\n",
    "\n",
    "- Visit [Langsmith](https://smith.langchain.com/)\n",
    "- Select the name of your project. Will be named `default` if you have not changed it\n",
    "- All executed LLM chains will be displayed. Click on one to view a breakdown of the steps.\n",
    "\n",
    "> **WARNING** Do not enable langsmith if the data / chain is confidential!\n",
    "\n",
    "![example](../images/langsmith.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the results so we can compare later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_results(\"basic.txt\", result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-techniques",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
